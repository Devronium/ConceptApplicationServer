#!/usr/local/bin/concept
include Worker.con
include TinBase.con
include TCPSocket.con
include UNIXSocket.con
include Poll.con
include DirectoryList.con
import standard.lib.cripto
import standard.math.rand
import standard.lang.cli
import standard.C.time

// <javascript support>
// include DUKJS.con instead of JS.con for using duktape instead of SpiderMonkey
include DUKJS.con
include SQLParser.con
include TinDBGlobalUDF.con
// </javascript support>

define TIN_VERSION	"1.0BETA"

define TIN_PORT		2668
define TIN_SYNCPORT	2669

define TIN_DEBUG	false
define TIN_FORCESYNC	true
define TIN_JOURNALING	true
define TIN_CLUSTER	false
define TIN_CLUSTER_LOG	true
define TIN_PING		false
define TIN_FULLOBJECT	true

define READ_BUFFER	0xFFFF

// 256M packet
define MAX_OP_SIZE	0xFFFFFFF
// 2k for auth
define MAX_NO_AUTH_SIZE	2048

define ERR_DB		-1
define ERR_DB_S		"Invalid db"
define ERR_COLLECTION	-2
define ERR_COLLECTION_S	"Invalid collection"
define ERR_INTERNAL	-3
define ERR_IO		-4
define ERR_IO_S		"Server I/O error"
define ERR_LOGIN	-5
define ERR_LOGIN_S	"Login required"
define ERR_USER_PASS	-6
define ERR_USER_PASS_S	"Invalid username or password"
define ERR_IN_CODE	-7
define ERR_NO_MEMBER_S	"No such function"
define ERR_CODE_RIGHTS	-8
define ERR_CODE_RIGHTS_S"Server code is disabled for this user"
define ERR_NOT_UNIQUE	-9
define ERR_NOT_UNIQUE_S	"Not unique"
define ERR_AGGREGATION	-10
define ERR_AGGREGATION_S"Invalid aggregation"
define ERR_JS_SCRIPT	-11
define ERR_TRIGGER	-12
define ERR_FILENAME	-13
define ERR_FILENAME_S	"Invalid filename"

define NOTIFY_MORE_DATA	1

define MAX_CACHED_QUERIES	128
define MAX_CACHED_USER		5000
define MAX_CACHE_OBJECT_LEN	50000
// max 8M per object
define MAX_CACHE_OBJECT_SIZE	8388608

define BUSY_FSYNC_LIMIT	 	10000
define WORKER_POLL_TIMEOUT	100

// max 5 connections per node
define SYNC_CONNECTIONS_MAX	5
define SYNC_CONFIRM_TIMEOUT	10000

// ring reload timeout (60 seconds)
define RING_RELOAD_TIMEOUT	60

// fsync every 5 seconds
define BUSY_FSYNC_TIMEOUT	5

// 16M log size
define LOG_CLEAN_LIMIT		16777216
define LOG_CLEAN_TIMEOUT	10
// sync every 100ms
define LOG_SYNC_US		100000

define REDUCE_TIMEOUT_S		5
define JS_TIMEOUT_S		0
define TRIGGER_TIMEOUT_S	60

define PING_CHECK		30
define PING_INTERVAL		300
define DROP_INTERVAL		600

class DummyObject {
}

class Triggers {
	var OnDelete;
	var OnInsert;
	var OnUpdate;

	var OnJSDelete;
	var OnJSInsert;
	var OnJSUpdate;

	var OnJSBeforeDelete;
	var OnJSBeforeInsert;
	var OnJSBeforeUpdate;
}

class SocketIOBase {
	static SendBuffer(socket, var to_send) {
		if (socket <= 0)
			return -1;
		var offset = 0;
		do {
			var res = SocketWrite(socket, to_send, false, "", 0, false, offset);
			if (res <= 0)
				return res;
			offset += res;
		} while (offset < length to_send);
		return offset;
	}

	static SendBufferWithSize(socket, to_send) {
		to_send = ToSize(length to_send) + to_send;
		var sent = SocketIOBase::SendBuffer(socket, to_send);
		if (sent != length to_send)
			return false;
		return true;
	}

	Send(socket, io = 0) {
		var to_send = BinarizeObject(this);
		to_send = ToSize(length to_send) + to_send;
		if (io) {
			AddWorkerData(io, BinarizeObject([socket, to_send, null, true]));
			return length to_send;
		}
		return SendBuffer(socket, to_send);
	}
}

class Q {
pragma used
	var q = "";
	var o = "Q";
	var d = "";
	var c = "";
	var f = null;

	var[] p;
	var[] x;

	var r;
	var io;
	var worker;
	var socket;
	var ctx_key;
	var cb;
	var aindex;
	var origp;
	var data;
	var child;
	var child_data;
	var notrig;
	var w;
	var version;

	MakeSafe() {
		this.r = null;
		this.io = null;
		this.worker = null;
		this.socket = null;
		this.ctx_key = null;
		this.cb = null;
		this.aindex = null;
		this.origp = null;
		this.data = null;
		this.child = null;
		this.child_data = null;
		this.notrig = null;
		this.w = 0;
		this.version = 0;
	}
}


class R extends SocketIOBase {
pragma used
	var id = -1;
	var q = "";
	var e = false;
	var et = "";
	var[] p;
}

class CacheData {
	var r;
	var timeout;

	CacheData(r, timeout) {
		this.r = r;
		this.timeout = time() + timeout;
	}
}

class SyncContext {
	var buffer = "";
	var key;
	var direction;
	var socket;

	SyncContext(ip, direction, sockfd) {
		this.key = ip;
		this.direction = direction;
		this.socket = sockfd;
	}
}

class SyncNode {
	var host;
	var name;
	var port;
	
	SyncNode(host, name = "", port = 2669) {
		this.host = host;
		this.name = name;
		this.port = port;
	}
}

class ConnectionContext {
	var buffer = "";
	var challenge = "";
	var auth = false;
	var code = true;
	var key;
	var subjects;
	var socket;
	var lastop;

	ConnectionContext(socket) {
		this.socket = socket;
		this.key = RandomInteger(1, 0xFFFFFFF);
		this.lastop = time();
	}

	AddSubject(subject) {
		if (!subjects)
			subjects = new [];

		subjects[subject] = subject;
	}

	RemoveSubject(subject) {
		if ((subjects) && (IsSet(subjects, subject))) {
			var keys = GetKeys(subjects);
			var new_subjects = new [];
			for (var i = 0; i < length subjects; i++) {
				var k = keys[i];
				if ((k) && (k != subject))
					new_subjects[k] = k;
			}
			if (new_subjects)
				subjects = new_subjects;
			else
				subjects = null;
		}
	}

	HasSubject(subject) {
		if (subjects)
			return (IsSet(subjects, subject) && (subjects[subject]));

		return false;
	}
}

class Journal {
	var f;
	var last_sync;
	var last_cleanup;
	var AutoClean = true;

	Journal(name) {
		if ((!FileExists(name)) || (_filesize(name) < 64)) {
			var header = pack("sU32:64", "Journal file", time());
			WriteFile(header, name);
			echo "Creating journal file $name\n";
		}
		this.Open(name);
	}

	Open(name) {
		if (f)
			this.Done();
		f = new File("ab");
		f.Name = name;
		if (f.Open()) {
			// no buffering
			setvbuf(f.Handle, 0);
			f.Seek(12);
			f.Read(var buf, 4);
			last_cleanup = unpack("U32", buf)[0];
			if (!LockFileBytes(f.Handle, F_SETLKW, 0, 0))
				echo "Error locking file $name\n";
			f.Seek(0, SEEK_END);
		} else {
			echo "Error opening $name\n";
			f = null;
		}
		last_sync = microseconds();
	}

	static Truncate(name, offset) {
		echo "Truncating invalid journal file at offset $offset\n";
		var f2 = new File("wb");
		f2.Name = name;
		if (f2.Open()) {
			try {
				f2.Truncate(offset);
				f2.Close();
			} catch (var exc) {
				echo "Error truncating file $name: $exc\n";
			}
		}
	}

	Read(newerthan = 0, no_unserialize = false) {
		var name = this.f.Name;
		this.Done();
		var f2 = new File("rb");
		f2.Name = name;
		var[] records;
		var truncate_at = 0;
		var hash_error = false;
		try {
			if (f2.Open()) {
				if (!f2.Seek(64))
					throw "Error in seek: ${_errno()}";
				do {
					truncate_at = f2.Tell();
					if (f2.Read(var buf, 27) != 27)
						break;
					var tell_offset = f2.Tell();
					var record = unpack("U32U64U64U16U8U32", buf);
					var record_size = record[2];
					var collection_size = record[3];
					if (record[0] >= newerthan) {
						var buffer;
						var collection;
						if (f2.Read(buffer, record_size) != record_size) {
							hash_error = true;
							break;
						}
						if (f2.Read(collection, collection_size) != collection_size) {
							hash_error = true;
							break;
						}
						if (record[5] != Murmur(buffer)) {
							hash_error = true;
							throw "Hash error";
							break;
						}
						var offset = record[1];
						if (offset <= 0)
							offset = -1;
						if (no_unserialize)
							records[length records] = ["time" => record[0], "flags" => record[4], "collection" => collection, '$' => offset, "data" => buffer];
						else
							records[length records] = ["time" => record[0], "flags" => record[4], "collection" => collection, '$' => offset, "data" => UnBinarizeObject(buffer)];
					} else {
						var seek_size = record_size + collection_size;
						if (!f2.Seek(tell_offset + seek_size))
							break;
					}
				} while (buf);
				f2.Close();
			} else
				f2 = null;
		} catch (var exc) {
			echo "Error in journal read: $exc\n";
			if (f2)
				f2.Close();
		}
		if (hash_error)
			Journal::Truncate(name, truncate_at);
		this.Open(name);
		return records;
	}

	static LeadingZero(i) {
		i = "" + i;
		while (length i < 4)
			i = "0" + i;
		return i;
	}

	static Check(name, records) {
		var f2 = new File("rb");
		f2.Name = name;
		var truncate_at = 0;
		var hash_error = false;
		try {
			if (f2.Open()) {
				if (!f2.Seek(12))
					throw "Error in seek: ${_errno()}";
				f2.Read(var buf, 4);
				if (length buf != 4)
					throw "Error in journal\n";
				var newerthan = unpack("U32", buf)[0];

				if (!f2.Seek(64))
					throw "Error in seek: ${_errno()}";
				do {
					truncate_at = f2.Tell();
					if (f2.Read(buf, 27) != 27)
						break;
					var tell_offset = f2.Tell();
					var record = unpack("U32U64U64U16U8U32", buf);
					var record_size = record[2];
					var collection_size = record[3];
					var flags = record[4];
					// insert or update
					if ((record[0] >= newerthan) && ((flags == 1) || (flags == 2) || (flags == 3))) {
						var buffer;
						var collection;
						if (f2.Read(buffer, record_size) != record_size) {
							hash_error = true;
							break;
						}
						if (record[5] != Murmur(buffer)) {
							hash_error  = true;
							throw "Hash failed";
							break;
						}
						if (f2.Read(collection, collection_size) != collection_size) {
							hash_error = true;
							break;
						}
						var offset = record[1];
						if (offset <= 0)
							offset = -1;

						var prec_record = records[collection];
						var record_time = record[0];
						if ((!prec_record) || (prec_record["time"] < record_time))
							records[collection] = ['$' => offset, "time" => record_time, "flags" => flags, "data" => buffer];
					} else {
						var seek_size = record_size + collection_size;
						if (!f2.Seek(tell_offset + seek_size))
							break;
					}
				} while (buf);
				f2.Close();
			} else
				f2 = null;
		} catch (var exc) {
			echo "Error in journal read: $exc\n";
			if (f2)
				f2.Close();
		}
		if (hash_error)
			Journal::Truncate(name, truncate_at);
		return records;
	}

	static StripPadding(obj) {
		if (obj) {
			var i = length obj;
			while (i > 0) {
				if (obj[i - 1] == "\x00")
					i--;
				else
					return SubStr(obj, 0, i);
			}
		}
		return obj;
	}

	static CheckAll(prefix = "journal", var broken_collections = null) {
		var i = 0;
		var[] records;
		broken_collections = new [];
		do {
			var fname = prefix + Journal::LeadingZero(i++);
			if (!FileExists(fname))
				break;
			Journal::Check(fname, records);
		} while (true);
		var keys = GetKeys(records);
		var db_is_ok = true;
		for (i = 0; i < length keys; i++) {
			var key = keys[i];
			if (key) {
				var obj = records[key];
				if (obj) {
					key = StrReplace(key, "\\", "/");
					var arr = StrSplit(key, "/");
					if (length arr == 2) {
						var db = arr[0];	
						var collection = arr[1];
						var op_type = obj["flags"];	
						var type = "";
						switch (op_type) {
							case 1:
								type = "INSERT";
								break;
							case 2:
								type = "UPDATE";
								break;
							case 3:
								type = "DELETE";
								break;
						}
						if (type) {
							var t = new BinFile(key, "r+b", DEFAULT_MODULO_SIZE, false);
							try {
								if (!t.Open()) {
									echo "Error opening file $key, errno: ${_errno()}\n";
									broken_collections[key] = key;
									db_is_ok = false;
									continue;
								}
								var id = obj['$'];
								t.Seek(id);
								var ref_obj = Journal::StripPadding(t.GetOne(id));
								t.Close();
								echo "Last write($type) for database $db, collection $collection was: ${obj['$']}, on ${ctime(obj["time"])}";
								if (obj["flags"] == 3) {
									if (ref_obj) {
										echo "Deleted object retrieved for id ($id). Database may be corrupted (file: $key)\n";
										db_is_ok = false;
										broken_collections[key] = key;
										continue;
									}
								} else {
									if (!ref_obj) {
										echo "Null object retrieved for id ($id). Database may be corrupted (file: $key)\n";
										db_is_ok = false;
										broken_collections[key] = key;
										continue;
									} else
									if (ref_obj != Journal::StripPadding(obj["data"])) {
										echo "Stored object is not the same as logged object. Database may be corrupted (file: $key)\n";
										db_is_ok = false;
										broken_collections[key] = key;
										continue;
									}
								}
								echo "Object is OK\n";
							} catch (var exc) {
								echo "I/O Error: $exc\n";
								return false;
							}
						}
					}
				}
			}
		}
		return true;
	}

	SafeWrite(f2, var buf) {
		if (f2.Write(buf) != length buf)
			throw "Write error in ${f2.Name}, errno: ${_errno()}\n";
	}

	Clean(olderthan) {
		try {
			if ((f) && (_filesize(f.Name) > LOG_CLEAN_LIMIT)) {
				var f2 = new File("wb");
				f2.Name = f.Name + ".clean";
				_unlink(f2.Name);
				if (f2.Open()) {
					f.Flush();
					f.Sync();
					var header = pack("sU32:64", "Journal file", time());
					if (f2.Write(header) != length header)
						throw "Error in cleanup log write ${f2.Name}, errno: ${_errno()}\n";

					if (!f.Seek(64))
						throw "Error in seek ${f.Name}, errno: ${_errno()}\n";
					do {
						if (f.Read(var buf, 27) != 27)
							break;
						var record = unpack("U32U64U64U16U8U32", buf);
						if (record[0] >= olderthan) {
							// copy from here
							SafeWrite(f2, buf);
							do {
								f.Read(buf, 0x100000);
								if (buf)
									SafeWrite(f2, buf);
							} while (buf);
							break;
						}
						var record_size = record[2] + record[3];
						if (!f.Seek(record_size, SEEK_CUR))
							break;
					} while (buf);
					var name = f.Name;
					f2.Flush();
					f2.Sync();
					f2.Close();
					this.Done();
					_unlink(name);
					if (rename(name + ".clean", name))
						throw  "Error in rename ${name}.clean to $name, errno: ${_errno()}\n";

					this.Open(name);
				}
			}
		} catch (var exc) {
			if (f)
				f.Seek(0, SEEK_END);
			echo exc;
		}
		last_cleanup = time();
	}

	DeleteOps(collection, id = -1) {
		try {
			if (f) {
				var name = f.Name;
				f.Flush();
				f.Sync();
				this.Done();

				var f3 = new File("rb");
				f3.Name = name;
				if (!f3.Open()) {
					this.Open(name);
					return;
				}
				var f2 = new File("wb");
				f2.Name = f3.Name + ".clean";
				_unlink(f2.Name);
				if (f2.Open()) {
					var header = pack("sU32:64", "Journal file", time());
					if (f2.Write(header) != length header)
						throw "Error in cleanup log write ${f2.Name}, errno: ${_errno()}\n";

					if (!f3.Seek(64))
						throw "Error in seek ${f3.Name}, errno: ${_errno()}\n";
					do {
						if (f3.Read(var buf, 27) != 27)
							break;
						var saved_offset = f3.Tell();
						var record = unpack("U32U64U64U16U8U32", buf);
						var offset = record[1];
						var record_size = record[2];
						var collection_size = record[3];
						if (!f3.Seek(record_size, SEEK_CUR))
							break;

						var log_collection = "";
						if (f3.Read(log_collection, collection_size) != collection_size)
							break;

						if ((log_collection != collection) && ((offset != id) || (id != -1))) {
							SafeWrite(f2, buf);
							if (!f3.Seek(saved_offset, SEEK_SET))
								break;
							var copy_size = record_size + collection_size;
							f3.Read(buf, copy_size);
							if (buf)
								SafeWrite(f2, buf);
						}
					} while (buf);
					f2.Flush();
					f2.Sync();
					f2.Close();
					f3.Close();

					_unlink(name);
					if (rename(name + ".clean", name))
						throw  "Error in rename ${name}.clean to $name, errno: ${_errno()}\n";

					this.Open(name);
				} else {
					f3.Close();
					this.Open(name);
				}
			}
		} catch (var exc) {
			if (f3)
				f3.Close();
			if (name) {
				this.Done();
				this.Open(name);
			}
			echo exc;
		}
		last_cleanup = time();
	}

	Reset() {
		try {
			var name = f.Name;
			this.Done();
			if (name) {
				_unlink(name);
				this.Open(name);
				var header = pack("sU32:64", "Journal file", time());
				if (f.Write(header) != length header)
					throw "Error in log reset ${name}, errno: ${_errno()}\n";
			}
		} catch (var exc) {
			echo exc;
		}
		last_cleanup = time();
	}

	Log(var op_bin, collection, offset, flags) {
		if ((op_bin) && (collection) && (f)) {
			try {
				if (offset < 0)
					offset = 0;

				var buffer = pack("U32U64U64U16U8U32", time(), offset, length op_bin, length collection, flags, Murmur(op_bin)) + op_bin + collection;
				if (f.Write(buffer) != length buffer) {
					echo "Error writing to log file '${f.Name}', errno: ${_errno()}\n";
					return false;
				}
				if (microseconds() - last_sync >= LOG_SYNC_US) {
					f.Flush();
					f.Sync();
					last_sync = microseconds();
				}
				if (AutoClean) {
					var now = time();
					if (now - last_cleanup > LOG_CLEAN_TIMEOUT)
						Clean(now - LOG_CLEAN_TIMEOUT);
				}
			} catch (var exc) {
				echo "Log error: $exc\n";
				return false;
			}
			return true;
		}
		return false;
	}

	Done() {
		if (f) {
			LockFileBytes(f.Handle, F_UNLCK, 0, 0);
			f.Close();
			f = null;
		}
	}
}

class QueryWorkerBase {
pragma used
	var[] buffers;
	var[] other_workers;
	var WorkerID;
	var syncworker;
	var oid_counter = -1;

	static WriterWorker(d, c, suggest_worker) {
		var key = "*" + d + "/" + c;
		return value Worker::GetSet(key, "worker", "" + suggest_worker);
	}

	static CacheKey(obj) {
		return sha1(BinarizeObject([obj.o, obj.f, obj.p, obj.x["start"], obj.x["len"], obj.x["descending"], obj.x["reduce"]]));
	}

	static InvalidateCache(d, c) {
		var key = "/" + d + "/" + c;
		Worker::Remove(key);
	}

	static Sync(obj, syncworker) {
		if (syncworker) {
			obj.w = time();
			AddWorkerData(syncworker, BinarizeObject(obj));
		}
	}

	static FullObjectSync(obj, o, oid, syncworker) {
		if ((syncworker) && (oid) && (typeof oid == "string")) {
			var q = new Q();
			q.o = "U";
			q.q = "*";
			q.d = obj.d;
			q.c = obj.c;
			q.f = o;
			q.p = ["\$oid" => oid];
			q.x["start"] = 0;
			q.x["len"] = 1;
			q.origp = obj.origp;
			QueryWorkerBase::Sync(q, syncworker);
			return true;
		}
		return false;
	}

	static InvalidateCacheSync(obj, syncworker = null) {
		QueryWorkerBase::InvalidateCache(obj.d, obj.c);
		QueryWorkerBase::Sync(obj, syncworker);
	}

	static Cache(obj, r, timeout = 0) {
		var key = "/" + obj.d + "/" + obj.c;
		var cache_key = QueryWorkerBase::CacheKey(obj);
		var cache_obj;
		if (length r < MAX_CACHE_OBJECT_LEN) {
			cache_obj = BinarizeObject(r);
			if (length cache_obj > MAX_CACHE_OBJECT_SIZE) {
				// don't cache large objects
				return;
			}
			if (Worker::Set(key, cache_key, cache_obj) > MAX_CACHED_QUERIES) {
				QueryWorkerBase::InvalidateCache(obj.d, obj.c);
				Worker::Set(key, cache_key, cache_obj);
			}
			if (timeout > 0) {
				key = "\$/" + obj.d + "/" + obj.c;
				cache_obj = BinarizeObject(new CacheData(r, timeout));
				if (length cache_obj > MAX_CACHE_OBJECT_SIZE)
					return;
				if (Worker::Set(key, cache_key, cache_obj) > MAX_CACHED_USER) {
					Worker::Remove(key);
					Worker::Set(key, cache_key, cache_obj);
				}
			}
		} else {
			Worker::Remove(key, cache_key);
		}
	}

	static CacheHit(obj, sock, current_worker) {
		var data;
		if ((obj.o == "Q") && ((!obj.x) || (!obj.x["map"]) || (!obj.x["explain"])) && (obj.c) && (obj.d)) {
			var key = "/" + obj.d + "/" + obj.c;
			var r;
			var data_obj;
			if (key) {
				var cache_key = QueryWorkerBase::CacheKey(obj);
				if ((obj.x) && (obj.x["cache"])) {
					var key2 = "\$/" + obj.d + "/" + obj.c;
					if (obj.x["renew"]) {
						Worker::Remove(key2, cache_key);
						data = null;
					} else
						data = Worker::Get(key2, cache_key);
					if (data) {
						data_obj = UnBinarizeObject(data);
						if (classof data_obj == "CacheData") {
							if (time() > data_obj.timeout) {
								Worker::Remove(key2, cache_key);
							} else {
								r = new R();
								r.q = obj.q;
								r.p = data_obj.r;
								if (sock < 0)
									return r;
								if ((!obj.io) || (obj.io != current_worker))
									r.Send(sock, obj.io);
								else
									r.Send(sock);
								return true;
							}
						}
					}
				}
				data = Worker::Get(key, cache_key);
				if (data) {
					data_obj = UnBinarizeObject(data);
					if ((data_obj) || (typeof data_obj == "array")) {
						r = new R();
						r.q = obj.q;
						r.p = data_obj;
						if (sock < 0)
							return r;
						if ((!obj.io) || (obj.io != current_worker))
							r.Send(sock, obj.io);
						else
							r.Send(sock);
						return true;
					}
				}
			}
		}
		return false;
	}

	static ReadJSON(json) {
		var data = ReadFile(json);
		if (data)
			return JSONDeserialize(data) ?? [ ];
		return [ ];
	}

	static NodeInfo(path = ".") {
		return QueryWorkerBase::ReadJSON(path + "/node.json");
	}

	static ClusterInfo(path = ".") {
		return QueryWorkerBase::ReadJSON(path + "/ring.json");
	}

	static WorkerMemory(extra = null) {
		var arr = WorkerMemoryInfo();
		arr["when"] = trim(StrReplace(ctime(time()), "\r", ""));
		if (extra) {
			var keys = GetKeys(extra);
			for (var i = 0; i < length keys; i++) {
				var k = keys[i];
				if (k)
					arr["." + k] = extra[k];
			}
		}
		return arr;
	}

	DoInfo(obj, sock, current_worker, storage_workers, idle_workers) {
		var r = new R();
		r.q = obj.q;
		var type = "";
		var obj_p = obj.p;
		if (typeof obj_p == "array")
			type = obj_p[0];

		if (typeof obj_p == "string")
			type = ToLower(obj_p);

		if (type == "databases") {
			r.p = new [];
			var list = DirectoryList::ListByType("./", S_IFDIR);
			for (var i = 0; i < length list; i++) {
				var d = list[i];
				if ((d) && (d[0] != "."))
					r.p[length r.p] = d;
			}
		} else
		if (type == "top") {
			r.p = getrusage(0);
			r.p["cw_connections"] = length buffers;
			r.p["cw_id"] = WorkerID;
		} else
		if (type == "collections") {
			r.p = new [];
			list = DirectoryList::ListByExt(obj.d + "/", [""], true);
			for (i = 0; i < length list; i++) {
				d = list[i];
				if (d) {
					var collection_data = [ ];

					var indexes = null;
					var idx = ReadFile(obj.d + "/" + d + ".idxn");
					if (idx) {
						indexes = UnBinarizeObject(idx);
						if (indexes)
							collection_data["index"] = indexes;
					}
					idx = ReadFile(obj.d + "/" + d + ".bm25");
					if (idx) {
						indexes = UnBinarizeObject(idx);
						if (indexes)
							collection_data["bm25"] = indexes;
					}
					if (collection_data)
						collection_data["name"] = d;
					else
						collection_data = d;
					r.p[d] = collection_data;
				}
			}
			if (r.p)
				r.p = KeySorted(r.p);
		} else
		if (type == "version")
			r.p = ["version" => TIN_VERSION, "forcesync" => TIN_FORCESYNC, "maxopsize" => MAX_OP_SIZE, "debug" => TIN_DEBUG, "js" => true, "cluster" => TIN_CLUSTER, "cluster.objectsync" => TIN_FULLOBJECT, "journaling" => TIN_JOURNALING, "ping" => TIN_PING];
		else
		if (type == "node")
			r.p = QueryWorkerBase::NodeInfo();
		else
		if (type == "ring")
			r.p = QueryWorkerBase::ClusterInfo();
		else
		if (type == "memory") {
			r.p = new [];
			var[] qw;
			var[] sw;
			r.p["StorageWorkers"] = sw;
			r.p["QueryWorkers"] = qw;
			var data_missing = false;

			Worker::Set("..//memory//..", WorkerID, BinarizeObject(QueryWorkerBase::WorkerMemory(["contexts" => length buffers])));
			for (i = 1; i <= length other_workers + 1; i++) {
				var wid = "QW$i";
				var data = Worker::Get("..//memory//..", wid);
				if (data)
					r.p[wid] = UnBinarizeObject(data);
				else
					data_missing++;
			}

			var worker_memory = BinarizeObject([-101]);
			for (i = 0; i < length other_workers; i++) {
				var w = other_workers[i];
				if (w)
					AddWorkerData(w, worker_memory);
			}
			var storage_count = 0;
			if (storage_workers) {
				for (i = 0; i < length storage_workers; i++) {
					w = storage_workers[i];
					if (w)
						AddWorkerData(w, worker_memory, -1);

					wid = "SW${i + 1}";
					data = Worker::Get("..//memory//..", wid);
					if (data)
						r.p[wid] = UnBinarizeObject(data);
					else
						data_missing++;
				}
				storage_count = length storage_workers;
			}
			if (idle_workers) {
				for (i = 0; i < length idle_workers; i++) {
					w = idle_workers[i];
					if (w)
						AddWorkerData(w, worker_memory, -1);
					wid = "SW${++storage_count}";
					data = Worker::Get("..//memory//..", wid);
					if (data)
						r.p["IDLE${i + 1}"] = UnBinarizeObject(data);
					else
						data_missing++;
				}
			}
			if (data_missing) {
				r.p["repeat"] = true;
				r.p["text"] = "Please run this query twice.";
				r.p["missing"] = data_missing;
			}
			r.p["cw_id"] = WorkerID;
		} else
		if (type == "gc") {
			echo "Garbage collector forced in worker #${WorkerID}\n";
			if (CheckReachability())
				echo " => unreachable objects found\n";
			var total_workers = 1;
			var worker_gc = BinarizeObject([-100]);
			var worker_gc_js = BinarizeObject([-9]);
			for (i = 0; i < length other_workers; i++) {
				w = other_workers[i];
				if (w) {
					AddWorkerData(w, worker_gc);
					total_workers++;
				}
			}
			if (idle_workers) {
				for (i = 0; i < length idle_workers; i++) {
					w = idle_workers[i];
					if (w) {
						AddWorkerData(w, worker_gc, -1);
						AddWorkerData(w, worker_gc_js, -1);
						total_workers++;
					}
				}
			}
			if (storage_workers) {
				for (i = 0; i < length storage_workers; i++) {
					w = storage_workers[i];
					if (w) {
						AddWorkerData(w, worker_gc, -1);
						AddWorkerData(w, worker_gc_js, -1);
						total_workers++;
					}
				}
			}

			r.p = new [];
			r.p["total_workers"] = total_workers;
			r.p["cw_id"] = WorkerID;
		} else
		if (type == "clear") {
			Worker::Clear();
			r.p = new [];
			r.p["cw_id"] = WorkerID;
		}


		if ((!obj.io) || (obj.io != current_worker))
			r.Send(sock, obj.io);
		else
			r.Send(sock);
	}

	DoPing(obj, sock, current_worker) {
		var r = new R();
		r.q = obj.q;
		r.p = obj.p;

		if ((!obj.io) || (obj.io != current_worker))
			r.Send(sock, obj.io);
		else
			r.Send(sock);
	}


	Dispatch(RequestAuth, ctx, sock, t, var next_worker, var next_a_worker, var next_idle_worker, storage_dbs, storage_workers, aggregation_workers, idle_workers, obj, current_worker, var buf, storage_dbs_w) {
		var priority = 0;
		var exc;
		if (obj.x) {
			var p2 = obj.x["priority"];
			if ((p2) && (typeof p2 == "numeric"))
				priority = p2;
		}
		switch (obj.o) {
			case "H":
				if ((ctx) && (!ctx.code)) {
					var r = new R();
					r.q = obj.q;
					r.e = ERR_CODE_RIGHTS;
					r.et = ERR_CODE_RIGHTS_S;
					if ((!obj.io) || (obj.io != current_worker))
						r.Send(sock, obj.io);
					else
						r.Send(sock);
					break;
				}
			case "A":
			case "Q":
			case "S":
			case "I":
			case "D":
			case "X":
			case "Y":
			case ".":
			case "*":
			case "R":
			case "L":
			case "W":
			case "U":
			case "Z":
			case "F":
			case "B":
			case "INFO":
			case "PING":
			case "PONG":
				if ((!ctx) || (ctx.auth)) {
					if (typeof obj.c != "string")
						obj.c = "";
					if ((!obj.d) || (typeof obj.d != "string") || (Pos(obj.d, "/") > 0) || (Pos(obj.d, "\\") > 0) || (obj.d == "..") || (obj.d == ".")) {
						r = new R();
						r.e = ERR_DB;
						r.et = ERR_DB_S;
						if ((!obj.io) || (obj.io != current_worker))
							r.Send(sock, obj.io);
						else
							r.Send(sock);
					} else
					if ((obj.c) && ((Pos(obj.c, "/") > 0) || (Pos(obj.c, "\\") > 0))) {
						r = new R();
						r.e = ERR_DB;
						r.et = ERR_DB_S;
						if ((!obj.io) || (obj.io != current_worker))
							r.Send(sock, obj.io);
						else
							r.Send(sock);
					} else {
						var storage;
						var ctx_key = obj.ctx_key;
						if ((!ctx_key) && (ctx)) {
							ctx_key = ctx.key;
							obj.ctx_key = ctx.key;
						}
						if (obj.o == "INFO") {
							DoInfo(obj, sock, current_worker, storage_workers, idle_workers);
							return;
						} else
						if (obj.o == "PING") {
							DoPing(obj, sock, current_worker);
							return;
						} else
						if (obj.o == "PONG") {
							echo "Received ping reply from ${sock}: ${round((microseconds() - obj.p[0])/1000000, 3)}s\n";
							// silently discard it
							return;
						}

						if ((!obj.io) && (!obj.cb) && (CacheHit(obj, sock, current_worker)))
							return;

						if (obj.o == "W") {
							storage = obj.worker;
							if (!storage) {
								storage = aggregation_workers[next_a_worker++];
								if (next_a_worker >= length aggregation_workers)
									next_a_worker = 0;
							}
							if (obj.socket <= 0) {
								obj.socket = sock;
								obj.io = current_worker;
							}
							if (!obj.r)
								obj.r = new R();
						} else
						if ((obj.o == "I") || (obj.o == "D") || (obj.o == "R") || (obj.o == "U") || (obj.o == "X") || (obj.o == "Y") || (obj.o == "Z") || (obj.o == "F")) {
							var storage_key = "" + obj.d + "/" + obj.c;
							storage = storage_dbs_w[storage_key];
							if (!storage) {
								storage = WriterWorker(obj.d, obj.c, storage_workers[next_worker++]);
								if (next_worker >= length storage_workers)
									next_worker = 0;
								storage_dbs_w[storage_key] = storage;
							}
						} else
						if (priority < 0) {
							// idle priority
							storage = idle_workers[next_idle_worker++];
							if (next_idle_worker >= length idle_workers)
								next_idle_worker = 0;
						} else {
							storage_key = "" + obj.d + "/" + obj.c;
							storage = storage_dbs[storage_key];
							if (!storage) {
								storage = storage_workers[next_worker++];
								if (next_worker >= length storage_workers)
									next_worker = 0;
								storage_dbs[storage_key] = storage;
							}
						}
						AddWorkerData(storage, BinarizeObject([sock, obj, current_worker, ctx_key]), priority);
					}
				} else
				if (RequestAuth)
					RequestAuth(ctx, sock, t, obj, buf);
				break;
			case "+":
				if ((!ctx) || (ctx.auth)) {
					// no response for add/remove subject
					if ((obj.p) && (typeof obj.p == "string"))
						ctx.AddSubject(obj.p);
				} else
				if (RequestAuth)
					RequestAuth(ctx, sock, t, obj, buf);
				break;
			case "-":
				if ((!ctx) || (ctx.auth)) {
					// no response for add/remove subject
					if ((obj.p) && (typeof obj.p == "string"))
						ctx.RemoveSubject(obj.p);				
				} else
					RequestAuth(ctx, sock, t, obj, buf);
				break;
			case "!":
				if ((!ctx) || (ctx.auth)) {
					if ((obj.p) && (typeof obj.p == "array") && (obj.p["w"])) {
						this.Notify(obj.p["w"], obj.p["o"], true, sock);
						this.Sync(obj, syncworker);
					}
				} else
				if (RequestAuth)
					RequestAuth(ctx, sock, t, obj, buf);
				break;
			case "G":
				if ((!ctx) || (ctx.auth)) {
					r = new R();
					r.q = obj.q;
					try {
						var inc = 1;
						if ((obj.p) && (IsSet(obj.p, "i")))
							inc = obj.p["i"];
						r.p = TinBase::Generator(obj.d, obj.p["g"], inc);
						this.Sync(obj, syncworker);
					} catch (exc) {
						r.e = ERR_IO;
						r.et = ERR_IO_S;
						echo "Exception: $exc\n";
					}
					if ((!obj.io) || (obj.io != current_worker))
						r.Send(sock, obj.io);
					else
						r.Send(sock);
				} else
				if (RequestAuth)
					RequestAuth(ctx, sock, t, obj, buf);
				break;
			case "oid":
				if ((!ctx) || (ctx.auth)) {
					r = new R();
					r.q = obj.q;
					try {
						if (oid_counter < 0)
							oid_counter = RandomInteger(0, 0xFFF0);
						if (oid_counter >= 0xFFFF)
							oid_counter = 0;
						r.id = TinDBOid::create(ClsPtr(this), WorkerID, oid_counter++);
						r.p = StringToHex(r.id);
						this.Sync(obj, syncworker);
					} catch (exc) {
						r.e = ERR_IO;
						r.et = ERR_IO_S;
						echo "Exception: $exc\n";
					}
					if ((!obj.io) || (obj.io != current_worker))
						r.Send(sock, obj.io);
					else
						r.Send(sock);
				} else
				if (RequestAuth)
					RequestAuth(ctx, sock, t, obj, buf);
				break;
		}
	}

	Notify(string subject, message, all_workers = false, socket = -1) {
		var notification = ["w" => subject, "o" => message];
		var r;
		for (var i = 0; i < length buffers; i++) {
			var ctx = buffers[i];
			if ((ctx) && (ctx.socket != socket) && (ctx.HasSubject(subject))) {
				if (!r) {
					// recycle r
					r = new R();
					r.q = "!";
					r.p = notification;
				}
				r.Send(ctx.socket);
			}
		}
		if (all_workers) {
			for (i = 0; i < length other_workers; i++) {
				var worker = other_workers[i];
				if (worker)
					AddWorkerData(worker, BinarizeObject([-7, subject, message]));
			}
		}
	}
}

class JSQueryWorker extends QueryWorkerBase {
	Execute(var data, var t, var next_worker, var next_a_worker, var next_idle_worker, var storage_dbs, var storage_workers, var aggregation_workers, var idle_workers, var current_worker, var storage_dbs_w) {
		if (data) {
			var job = UnBinarizeObject(data);
			var socket = job[0];
			var remote_obj = job[1];
			if (socket == -7) {
				this.Notify(remote_obj, job[2], true);
			} else
			if (socket == -100) {
				echo "Garbage collector forced in JS query worker\n";
				if (CheckReachability())
					echo " => unreachable objects found\n";
			} else
			if (socket == -101) {
				Worker::Set("..//memory//..", WorkerID, BinarizeObject(QueryWorkerBase::WorkerMemory()));
			} else
			if ((remote_obj) && (classof remote_obj == "Q") && (remote_obj.o) && (remote_obj.cb))
				Dispatch(null, null, -1, null, next_worker, next_a_worker, next_idle_worker, storage_dbs, storage_workers, aggregation_workers, idle_workers, remote_obj, current_worker, "", storage_dbs_w);
			remote_obj = null;
			job = null;
		}
	}

	JSQueryWorker(n) {
		n = UnBinarizeObject(n);
		WorkerID = n[0];
		var storage_workers = n[1];
		var idle_workers =  n[2];
		var[] storage_dbs;
		var[] storage_dbs_w;
		var next_worker = 0;
		var next_a_worker = 0;
		var next_idle_worker = 0;

		var AccessDB = new TinBase("sys");
		AccessDB.Mode = "rb";
		var t = AccessDB["users"];
		t.Open();
		var current_worker = CurrentWorker();

		Worker::Pending(var aggregation_data, -1);
		var ref_data = UnBinarizeObject(aggregation_data);
		var aggregation_workers = ref_data[0];
		var all_query_workers = ref_data[1];
		syncworker = ref_data[2];

		if (all_query_workers) {
			for (var j = 0; j < length all_query_workers; j++) {
				var q_worker = all_query_workers[j];
				if ((q_worker) && (q_worker != current_worker))
					other_workers[length other_workers] = q_worker;
			}
		}
		var data;
		while (true) {
			data = "";
			Worker::Pending(data, -1);
			Execute(data, t, next_worker, next_a_worker, next_idle_worker, storage_dbs, storage_workers, aggregation_workers, idle_workers, current_worker, storage_dbs_w);
		}
	}
}

class QueryWorker extends QueryWorkerBase {
pragma used
	var[] sockets;
	var poll;
	var lastping;

	RemoveSocket(sock) {
		var res = new [];
		var buffers = this.buffers;
		var buf = new [];
		var sockets = this.sockets;
		var keys = GetKeys(buffers);
		var sock_key = "" + sock;
		for (var i = 0; i < length sockets; i++) {
			var s = sockets[i];
			if ((s) && (s != sock))
				res[length res] = s;
			var k = keys[i];
			if ((k) && (k != sock_key))
				buf[k] = buffers[k];
		}
		this.poll.remove(sock);
		SocketClose(sock);
		this.sockets = res;
		this.buffers = buf;
	}

	RandomString(len) {
		var res = "";
		while (length res < len) {
			var c = RandomInteger(33, 126);
			if ((c != '\r') && (c != '\n') && (c != ';'))
				res += chr(c);
		}
		return res;
	}

	RequestAuth(ctx, sock, var t, obj, var buf) {
		var r = new R();
		r.q = obj.q;
		if (ctx.challenge) {
			if (obj.o == "A") {
				var username = obj.p["u"];
				var digest = obj.p["d"];
				var db = obj.p["db"];
				try {
					t.Lock();
					var user = t.Query(["username" => username])[0];
					t.Unlock();
					if (user) {
						var d2 = sha256(username + ":" + ctx.challenge + ":" + user["password"]);
						var user_db = user["db"];
						var db_valid = false;
						if (user_db) {
							if (typeof user_db == "array") {
								for (var db_i = 0; db_i < length user_db; db_i++) {
									var u_db = user_db[db_i];
									if ((u_db) && (typeof u_db == "string") && (u_db == user_db)) {
										db_valid = true;
										break;
									}
								}
							} else
							if (user_db == db)
								db_valid = true;
						} else
							db_valid = true;
						if ((user) && (d2 == digest) && (db_valid)) {
							ctx.auth = true;
							if (IsSet(user, "code"))
								ctx.code = (user["code"] == 1);
							r.id = 0;
							r.p["ok"] = true;
							r.Send(sock);
						}
					}
				} catch (var exc) {
					t.ReleaseLock();
					echo "Exception: $exc\n";
				}
			}
			if (!ctx.auth) {
				r.e = ERR_USER_PASS;
				r.et = ERR_USER_PASS_S;
				r.Send(sock);
				RemoveSocket(sock);
				buf = "";
			}
		} else {
			ctx.challenge = RandomString(48);
			r.e = ERR_LOGIN;
			r.et = ERR_LOGIN_S;
			r.p["c"] = ctx.challenge;
			r.p["m"] = "sha256";
			r.Send(sock);
		}
	}

	KeepAlivePing() {
		if (!sockets)
			return;
		var now = time();
		var buffers_copy = buffers;
		for (var i = 0; i < length buffers_copy; i++) {
			var ctx = buffers_copy[i];	
			if (ctx) {
				try {
					if (now - ctx.lastop > DROP_INTERVAL) {
						RemoveSocket(ctx.socket);
					} else
					if (now - ctx.lastop > PING_INTERVAL) {
						var r = new R();
						r.q = "PING";
						r.p = [microseconds()];
						if (r.Send(ctx.socket) <= 0)
							RemoveSocket(ctx.socket);
						else
							echo "Sent ping to ${ctx.socket}\n";
					}
				} catch (var exc) {
					echo "Ping exception: $exc\n";
				}
			}
		}
	}

	Execute(var data, var sleep_value, var t, var next_worker, var next_a_worker, var next_idle_worker, var storage_dbs, var storage_workers, var aggregation_workers, var idle_workers, var current_worker, var storage_dbs_w) {
		var timeout = sleep_value;
		if (data) {
			timeout = 0;
			var job = UnBinarizeObject(data);
			if (job) {
				var socket = job[0];
				var remote_obj = job[1];
				if (socket > 0) {
					if (job[3]) {
						// is postback
						var socket_key = "" + socket;
						// check if socket is still valid
						if (IsSet(buffers, socket_key)) {
							var len = SocketIOBase::SendBuffer(socket, remote_obj);
							if (len != length remote_obj) {
								// connection was dropped
								RemoveSocket(socket);
							}
						}
					} else {
						var post_key = remote_obj;//job[1];
						if (post_key) {
							var sock_key = "" + socket;
							if (IsSet(buffers, sock_key)) {
								var post_back_ctx = buffers[sock_key];
								if ((post_back_ctx) && (post_back_ctx.key == post_key)) {
									var post_back_r = job[2];
									post_back_r.Send(socket);
									delete post_back_r;
									delete job;
								}
							}
						} else {
							sockets[length sockets] = socket;
							this.poll.add(socket);
							buffers["" + socket] = new ConnectionContext(socket);
						}
					}
				} else
				if (socket == -7) {
					this.Notify(remote_obj, job[2]);
				} else
				if (socket == -100) {
					echo "Garbage collector forced in worker #${WorkerID}\n";
					if (CheckReachability())
						echo " => unreachable objects found\n";
				} else
				if (socket == -101) {
					Worker::Set("..//memory//..", WorkerID, BinarizeObject(QueryWorkerBase::WorkerMemory(["sockets" => length sockets, "contexts" => length buffers])));
				} else
				if (remote_obj) {
					if (((remote_obj) && (classof remote_obj == "Q") && (remote_obj.o) && ((remote_obj.o == "W") || (remote_obj.o == "Q"))) || (socket == -20))
						Dispatch(RequestAuth, null, -1, null, next_worker, next_a_worker, next_idle_worker, storage_dbs, storage_workers, aggregation_workers, idle_workers, remote_obj, current_worker, "", storage_dbs_w);
				}
				remote_obj = null;
				job = null;
			}
		}
		if (!sockets) {
			sleep_value = -1;
			return;
		}
		if (timeout < 0)
			timeout = 0;
		var outsockets = this.poll.wait(timeout);
		if (/*(SocketPoll(sockets, var outsockets, timeout)) &&*/ (outsockets)) {
			for (var i = 0; i < length outsockets; i++) {
				var sock = outsockets[i];
				var buffer = "";
				var buf2 = "";
				var obj = null;
				var res = SocketRead(sock, buffer, READ_BUFFER);
				if (res > 0) {
					var ctx = buffers["" + sock];
					ctx.lastop = time();
					var buf = ctx.buffer + buffer;
					// deserialize
					while (buf) {
						var to_read = FromSize(buf, var bytes);
						if (to_read <= MAX_OP_SIZE) {
							if ((!ctx.auth) && (to_read > MAX_NO_AUTH_SIZE)) {
								// drop socket ... packet to large for unauthorized user
								RemoveSocket(sock);
								break;
							}
							if ((to_read > 0) && (to_read <= length buf - bytes)) {
								buf2 = SubStr(buf, bytes, to_read);
								buf = SubStr(buf, to_read + bytes);
								obj = UnBinarizeObject(buf2);
 								if ((obj) && (classof obj == "Q") && (obj.o)) {
									obj.MakeSafe();
									Dispatch(RequestAuth, ctx, sock, t, next_worker, next_a_worker, next_idle_worker, storage_dbs, storage_workers, aggregation_workers, idle_workers, obj, current_worker, buf, storage_dbs_w);
								}
							} else {
								ctx.buffer = buf;
								break;
							}
							ctx.buffer = buf;
						} else {
							// drop socket ... packet to large
							RemoveSocket(sock);
							break;
						}
					}
				} else
					RemoveSocket(sock);
			}
			sleep_value = 2;
			return;
		}
		if (sockets) {
			if ((TIN_PING) && (time() - lastping > PING_CHECK)) {
				KeepAlivePing();
				lastping = time();
			}
			sleep_value = 2; // Sleep(2);
		} else
			sleep_value = -1; // Sleep(5);
	}

	QueryWorker(n) {
		n = UnBinarizeObject(n);
		WorkerID = n[0];
		var storage_workers = n[1];
		var idle_workers =  n[2];
		var[] storage_dbs;
		var[] storage_dbs_w;
		var next_worker = 0;
		var next_a_worker = 0;
		var next_idle_worker = 0;

		this.poll = new Poll();

		var AccessDB = new TinBase("sys");
		AccessDB.Mode = "rb";
		var t = AccessDB["users"];
		t.Open();
		var current_worker = CurrentWorker();

		Worker::Pending(var aggregation_data, -1);
		var ref_data = UnBinarizeObject(aggregation_data);
		var aggregation_workers = ref_data[0];
		var all_query_workers = ref_data[1];
		syncworker = ref_data[2];
		if (all_query_workers) {
			for (var j = 0; j < length all_query_workers; j++) {
				var q_worker = all_query_workers[j];
				if ((q_worker) && (q_worker != current_worker))
					other_workers[length other_workers] = q_worker;
			}
		}
		var sleep_value = 0;
		var data;
		while (true) {
			Worker::PendingAll(data, sleep_value);
			if (data) {
				for (var i = 0; i < length data; i++)
					Execute(data[i], sleep_value, t, next_worker, next_a_worker, next_idle_worker, storage_dbs, storage_workers, aggregation_workers, idle_workers, current_worker, storage_dbs_w);
				data = null;
			} else
				Execute(null, sleep_value, t, next_worker, next_a_worker, next_idle_worker, storage_dbs, storage_workers, aggregation_workers, idle_workers, current_worker, storage_dbs_w);
		}
	}
}

class StorageWorker {
pragma used
	var[] managed_dbs;
	var[] objects;
	var[] members;
	var[] _triggers;
	var non_flushed_writes = 0;
	var last_flush = 0;
	var[] other_workers;
	var counter;
	var workerid;
	var journal;
	var[] jsctx;
	var jsworker;
	var syncworker;
	var machineid;
	var globaludf;

	GetDelegate(dbobject, db, name, var objects, var members, r = null) {
		var arr = StrSplit(name, ".");
		var member;
		if (length arr == 2) {
			var classname = arr[0];
			var mname = arr[1];
			var key = "$db.$classname";		
			var key_mname = "$key.$mname";
			member = members[key_mname];
			if (!member) {
				var handler = objects[key];
				if (!handler) {
					handler = CreateObject2(classname, dbobject);
					if (handler)
						objects[key] = handler;
				}
				if (handler) {
					if (!GetMember(handler, mname, member, true))
						member = null;
				}
			}

			if ((r) && (!member)) {
				r.e = ERR_IN_CODE;
				r.et = ERR_NO_MEMBER_S;
			}
		}
		return member;
	}

	Flush(flush_index = true) {
		non_flushed_writes = 0;
		last_flush = time();
		for (var i = 0; i < length managed_dbs; i++) {
			var db = managed_dbs[i];
			if (db) {
				try {
					db.Flush(flush_index);
				} catch (var exc) {
					echo "Flush error: $exc\n";
				}
			}
		}
	}

	ResetTriggers(all_workers = false) {
		if (all_workers) {
			for (var i = 0; i < length other_workers; i++) {
				var worker = other_workers[i];
				if (worker)
					AddWorkerData(worker, BinarizeObject([-10]));
			}
		} else {
			for (i = 0; i < length managed_dbs; i++) {
				var db = managed_dbs[i];
				if (db) {
					echo "Resetting triggers in worker #${CurrentWorker()}\n";
					var t2 = db.ReadTriggers();
					if (typeof t2 == "array")
						_triggers = t2;
					// just once
					break;
				}
			}
		}
	}

	ResetJS(all_workers = false) {
		if (all_workers) {
			for (var i = 0; i < length other_workers; i++) {
				var worker = other_workers[i];
				if (worker)
					AddWorkerData(worker, BinarizeObject([-9]));
			}
		} else {
			echo "Resetting JS context in worker #${CurrentWorker()}\n";
			for (i = 0; i < length managed_dbs; i++) {
				var db = managed_dbs[i];
				if (db)
					this.ResetJSIfDefined(db);
			}
		}
	}

	ResetDBS(all_workers = false) {
		if (!all_workers)
			echo "\t * Resetting storage worker #${CurrentWorker()}\n";
		else
			echo "Resetting storage worker #${CurrentWorker()}\n";
		for (var i = 0; i < length managed_dbs; i++) {
			var db = managed_dbs[i];
			if (db) {
				db.Reset();
			}
		}
		if (all_workers) {
			echo "\tresetting child workers ...\n";
			for (i = 0; i < length other_workers; i++) {
				var worker = other_workers[i];
				if (worker)
					AddWorkerData(worker, BinarizeObject([-8]));
			}
		}
	}


	DeleteFromJournal(obj) {
		if (journal) {
			var journal_key = "${obj.d}/${obj.c}";
			var id = -1;
			if ((obj.p) && (IsSet(obj.p, '$'))) {
				id = obj.p['$'];
				if (id <= 0)
					id = -1;
			}
			journal.DeleteOps(journal_key, id);
			var bin_data = BinarizeObject([-102, journal_key, id]);
			for (var i = 0; i < length other_workers; i++) {
				var worker = other_workers[i];
				if (worker)
					AddWorkerData(worker, bin_data);
			}
		}
	}


	ForceNumber(val, not_zero = false) {
		if (typeof val != "numeric") {
			if (typeof val == "string")
				val = value val;
			else
				val = 0;
		}
		if ((not_zero) && (!val) )
			val = 0.0000000001;
		return val;
	}

	ForceEval(o, val, o_k, k) {
		var bin_o = StringToHex(BinarizeObject(o));
		var code = "import standard.lib.str\nimport standard.C.math\nimport standard.lang.serialize\nimport standard.lib.cripto\nimport standard.math.rand\nimport standard.C.time\n";
		code += "class Main{Main(){\n";
		code += "var o = UnBinarizeObject(HexToString(\"$bin_o\"));\n";
		code += "if (!o)\n";
		code += "return 0;\n";
		code += "return StringToHex(BinarizeObject([($val)]));\n";
		code += "}}\n";
		var val2 = eval(code, false);
		if (val2) {
			val2 = UnBinarizeObject(HexToString(val2));
			if (val2)
				return val2[0];
		}
		return o_k;
	}

	tin(dbname, array sql, callback = null) {
		var db;
		for (var i = 0; i < length sql; i++) {
			var q = sql[i];
			if (q) {
				if (IsSet(q, "SELECT")) {
					var fields = q["SELECT"];
					if ((fields) && (length fields == 1) && (fields[0] == "*"))
						fields = null;	

					var collection = q["FROM"];
					var where = q["WHERE"] ?? [ ];
					var mod = q["MOD"];
					var join = q["JOIN"];

					var q_obj = new Q;
					q_obj.q = "TinDBInternal";
					q_obj.o = "Q";
					q_obj.f = fields;
					q_obj.d = dbname;
					q_obj.c = collection;
					q_obj.x["start"] = q["OFFSET"];
					q_obj.x["len"] = q["LIMIT"];
					q_obj.x["descending"] = (q["ORDER"] == -1);
					q_obj.x["reduce"] = q["REDUCE"];
					q_obj.x["priority"] = q["PRIORITY"];
					var cache_timeout = q["CACHE"]; 
					if (cache_timeout > 0)
						q_obj.x["cache"] = cache_timeout;
					if (q["EXPLAIN"])
						q_obj.x["explain"] = true;
					if (mod) {
						var agg = mod[0][0];
						if (agg == "COUNT")
							q_obj.p = ["#" => where];
					} else
					if (join) {
						q_obj.o = "W";
						var join_query = [where];	
						for (var j = 0; j < length join; j++)
							join_query[length join_query] = join[j];

						q_obj.p = join_query;
					} else {
						q_obj.p = where;
					}
					try {
						if (!db)
							db = this.EnsureDB(q_obj);
						var js = this.EnsureJSContext(db);
						var r = QueryWorker::CacheHit(q_obj, -1, null);
						if (!r) {
							if ((js.UserData[1]) || (join)) {
								if (callback) {
									q_obj.cb = ["db" => dbname, "cb" => callback, "io" => CurrentWorker()];
									AddWorkerData(jsworker, BinarizeObject([-1, q_obj, "", ""]));
									return ["async" => true];
								}
								// dummy call
								if (join)
									return ["async" => true];
								return ["error" => "Locked. Use asynchronous query to resolve this."];
							}
							r = this.Execute([-1, q_obj, "", ""]);
						}
						if (callback) {
							if (js) {
								// js.variable("out", r.p);
								// js.script(callback + "(out)", "callback.js");
								js.timeout(JS_TIMEOUT_S);
								js.call(callback, [r.p]);
							}
							return ["async" => true];
						} else
							return ["data" => r.p];
					} catch (var exc) {
						return ["error" => exc];
					}
				} else
				if (IsSet(q, "INSERT")) {
					collection = q["INTO"];
					var obj = q["INSERT"];
					if (!obj)
						return ["error" => "Nothing to insert"];
					var keys = GetKeys(obj);

					q_obj = new Q;
					q_obj.q = "TinDBInternal";
					q_obj.o = "I";
					q_obj.d = dbname;
					q_obj.c = collection;
					q_obj.x["multi"] = ((!keys) || (!keys[0]));
					q_obj.x["priority"] = q["PRIORITY"];
					if (!callback)
						q_obj.x["silent"] = true;

					q_obj.p = obj;
					try {
						if (!db)
							db = this.EnsureDB(q_obj);
						js = this.EnsureJSContext(db);
						q_obj.cb = ["db" => dbname, "cb" => callback, "io" => CurrentWorker()];
						AddWorkerData(jsworker, BinarizeObject([-1, q_obj, "", ""]));
						return ["async" => true];
					} catch (exc) {
						return ["error" => exc];
					}
				} else
				if (IsSet(q, "UPDATE")) {
					collection = q["UPDATE"];
					where = q["WHERE"] ?? [ ];

					q_obj = new Q;
					q_obj.q = "TinDBInternal";
					q_obj.o = "U";
					q_obj.f = q["SET"];
					q_obj.d = dbname;
					q_obj.c = collection;
					q_obj.x["start"] = q["OFFSET"];
					q_obj.x["len"] = q["LIMIT"];
					q_obj.x["descending"] = (q["ORDER"] == -1);
					q_obj.x["reduce"] = q["REDUCE"];
					q_obj.x["priority"] = q["PRIORITY"];
					if (!callback)
						q_obj.x["silent"] = true;

					q_obj.p = where;
					try {
						if (!db)
							db = this.EnsureDB(q_obj);
						js = this.EnsureJSContext(db);
						q_obj.cb = ["db" => dbname, "cb" => callback, "io" => CurrentWorker()];
						AddWorkerData(jsworker, BinarizeObject([-1, q_obj, "", ""]));
						return ["async" => true];
					} catch (exc) {
						return ["error" => exc];
					}
				} else
				if (IsSet(q, "DELETE")) {
					collection = q["FROM"];
					where = q["WHERE"] ?? [ ];

					q_obj = new Q;
					q_obj.q = "TinDBInternal";
					q_obj.o = "D";
					q_obj.d = dbname;
					q_obj.c = collection;
					q_obj.x["start"] = q["OFFSET"];
					q_obj.x["len"] = q["LIMIT"];
					q_obj.x["descending"] = (q["ORDER"] == -1);
					q_obj.x["reduce"] = q["REDUCE"];
					q_obj.x["priority"] = q["PRIORITY"];
					if (!callback)
						q_obj.x["silent"] = true;

					q_obj.p = where;
					try {
						if (!db)
							db = this.EnsureDB(q_obj);
						js = this.EnsureJSContext(db);
						q_obj.cb = ["db" => dbname, "cb" => callback, "io" => CurrentWorker()];
						AddWorkerData(jsworker, BinarizeObject([-1, q_obj, "", ""]));
						return ["async" => true];
					} catch (exc) {
						return ["error" => exc];
					}
				} else
				if (IsSet(q, "SEQUENCE")) {
					var generator_name = q["SEQUENCE"];
					var increment = q["INCREMENT"];

					q_obj = new Q;
					q_obj.q = "TinDBInternal";
					q_obj.o = "G";
					q_obj.d = dbname;
					q_obj.c = collection;
					q_obj.x["priority"] = q["PRIORITY"];

					q_obj.p = ["g" => generator_name, "i" => increment];
					try {
						if (!db)
							db = this.EnsureDB(q_obj);
						js = this.EnsureJSContext(db);
						q_obj.cb = ["db" => dbname, "cb" => callback, "io" => CurrentWorker()];
						AddWorkerData(jsworker, BinarizeObject([-1, q_obj, "", ""]));
						return ["async" => true];
					} catch (exc) {
						return ["error" => exc];
					}
				} else
					return ["error" => "Unsupported SQL syntax"];
			}
		}
		return ["data" => null];
	}

	jsquery(string dbname, string query_str, parameters = null, callback = null) {
		var cb;
		if (typeof callback == "array") {
			cb = callback[".function"];
			if ((!cb) || (typeof cb != "string"))
				return "db.query: Callback must be a named function";
		}

		var sql = SQLParser::parse(query_str, var err, parameters);
		if ((sql) && (!err))
			return tin(dbname, sql, cb);

		return ["error" => err];
	}

	jsnotify(subject, data) {
		AddWorkerData(jsworker, BinarizeObject([-7, subject, data]));
	}

	jsprint(text) {
		echo text;
		echo "\n";
	}

	protected jstrycatch(data) {
		return "try { " + data + " } catch (__jsexc) { print(__jsexc); }";
	}

	SetJSContext(db, js, skip_code = false) {
		js.variable("___internal__tindb__currentdb", db.Use);
		js.wrap(jsquery, "___internal__tindb__query");
		js.wrap(jsnotify, "___internal__tindb__notify");
		js.wrap(jsprint, "print");
		try {
			js.timeout(0);
			js.script('var console = {log: print, warn: print, error: print }; var db = {query: function(query, parameters, callback) { var data = ___internal__tindb__query(___internal__tindb__currentdb, query, parameters, callback); if (data.error) throw data.error; return data.data; }, "notify": function(subject, data) { ___internal__tindb__notify(subject, data); }, "name": ___internal__tindb__currentdb }; var alert = print;', "db.js");
			// global Concept JS functions
			globaludf = new TinDBGlobalUDF(js);

			var data = ReadFile("server.js");
			if (data)
				js.script(jstrycatch(data), "server.js");
			if (!skip_code) {
				data = db.ReadJS();
				if (data)
					js.script(jstrycatch(data));
			}
		} catch (var exc) {
			echo "Unhandled JSError: $exc\n";
		}
	}

	ResetJSContext(db, skip_code = false) {
		if (!db)
			return null;

		var dbname = db.Use;
		var js = jsctx[dbname];
		if (js) {
			if (js.UserData[1]) {
				echo "Cannot reset JS context (locked) ...\n";
				return js;
			}
			// recursive
			if (js.UserData[0]) {
				echo "Cannot reset JS context (recursive call) ...\n";
				return js;
			}
			js.reset();
		} else {
			js = new JS();
			js.UserData = new [];
			jsctx[dbname] = js;
		}
		SetJSContext(db, js, skip_code);
		return js;
	}

	EnsureJSContext(db) {
		if (!db)
			return null;

		var dbname = db.Use;
		var js = jsctx[dbname];
		if (js)
			return js;

		js = new JS();
		js.UserData = new [];
		jsctx[dbname] = js;
		SetJSContext(db, js);
		return js;
	}

	ResetJSIfDefined(db) {
		if (!db)
			return null;

		if (IsSet(jsctx, db.Use))
			ResetJSContext(db);
	}

	JSTrigger(db, var o, code, allow_modify = false) {
		try {
			var js = EnsureJSContext(db);
			js.UserData[1] = true;
			js.UserData[0]++;
			js.variable("self", o);
			js.timeout(TRIGGER_TIMEOUT_S);
			var new_o = js.script(jstrycatch(code));
			js.gc();
			if ((allow_modify) && (new_o)) {
				if (typeof new_o == "array") {
					var keys = GetKeys(new_o);
					if ((keys) && (keys[0]))
						o = new_o;
				}
			}
		} catch (var exc) {
			js.UserData[1] = false;
			js.UserData[0]--;
			js.gc();
			return exc;
		}
		js.UserData[1] = false;
		js.UserData[0]--;
		return null;
	}

	JSCall(ctx, r) {
		try {
			if ((ctx) && (r)) {
				var[] data;
				if (r.e) {
					data["error"] = r.et;
				} else {
					if ((!r.p) && (r.id))
						data["data"] = r.id;
					else
						data["data"] = r.p;
				}

				var dbname = ctx["db"];
				var callback = ctx["cb"];
				var db;
				if ((dbname) && (callback))
					db = this.EnsureDBByName(dbname);
				if (db) {
					var js = EnsureJSContext(db);
					// js.variable("out", data);
					// js.script(callback + "(out)", "callback.js");
					js.timeout(JS_TIMEOUT_S);
					js.call(callback, [data]);
					js.gc();
				}
			}
		} catch (var exc) {
			echo "JSCall exception: $exc\n";
		}
	}

	EnsureDBByName(dbname) {
		var db = managed_dbs[dbname];
		if (!db) {
			if ((Pos(dbname, "/") > 0) || (Pos(dbname, "\\") > 0))
				throw "DB name error";
			var log = null;
			if (journal)
				log = journal.Log;
			db = new TinBase(dbname, log);
			if (TIN_DEBUG)
				db.DebugPath = dbname + "/";
			managed_dbs[dbname] = db;
			var code = db.ReadHandler();
			if (code) {
				var load_err = IncludeCode(code);
				if (load_err) {
					echo "Stored handler loading error:\n$load_err\n";
					load_err = "";
				}
				code = "";
			}
			if (!_triggers) {
				var t2 = db.ReadTriggers();
				if (typeof t2 == "array")
					_triggers = t2;	
			}
		}
		return db;
	}

	EnsureDB(obj) {
		return EnsureDBByName(obj.d);
	}

	DeleteFile(db, collection, name) {
		var filename = "$db/fs/$collection/$name.bin";
		if (_unlink(filename))
			return false;
		return true;
	}

	RemoveCollectionFS(db, collection) {
		return DirectoryList::Delete("$db/fs/$collection/");
	}

	StoreFile(db, collection, name, offset, var buffer) {
		_mkdir("$db/fs");
		_mkdir("$db/fs/$collection");
		var filename = "$db/fs/$collection/$name.bin";
		if ((!FileExists(filename)) || (offset == -2))
			WriteFile("", filename);
		var f = new File("r+b");
		f.Name = filename;
		if (f.Open()) {
			try {
				if (offset > 0) {
					var fs = f.Size;
					var delta = offset - fs;
					if (delta > 0) {
						f.Seek(fs);
						f.Write(pack(":$delta"));
					}
					f.Seek(offset);
				}
				if (offset < 0)
					f.Seek(0, SEEK_END);

				var written = f.Write(buffer);
				f.Close();
				return written;
			} catch (var exc) {
				echo "Error writing to file $filename: $exc\n";
				return -1;
			}
		} 
		return -1;
	}

	GetFileBuffer(db, collection, name, offset, size) {
		var[] data;
		var filename = "$db/fs/${collection}/$name.bin";
		data["name"] = name;
		if (FileExists(filename)) {
			data["exists"] = true;
			if ((size < 0) && (offset <= 0)) {
				data["buffer"] = ReadFile(filename);
				data["size"] = length data["buffer"];
			} else {
				var f = new File("rb");
				if (f.Open()) {
					try {
						data["size"] = f.Size;
						if (offset > 0) {
							data["offset"] = offset;
							if (!f.Seek(offset)) {
								data["buffer"] = "";
								f.Close();
								return data;
							}
						}
						if (size > 0) {
							data["chunk"] = f.Read(var buffer, size);
							data["buffer"] = buffer;
						}
						f.Close();
					} catch (var exc) {
						echo "Error reading file $filename: $exc\n";
					}
				}
			}
		} else {
			data["exists"] = false;
			data["buffer"] = "";
		}
		return data;
	}

	static CopyKeys(self, new_vals) {
		var self_keys = GetKeys(self);
		for (var i = 0; i < length self_keys; i++) {
			var self_key = self_keys[i];
			if (self_key)
				new_vals[self_key] = self[self_key];
		}
	}

	ManagedCollections() {
		var[] res;
		for (var i = 0; i < length managed_dbs; i++) {
			var db = managed_dbs[i];
			if (db) {
				var keys = db.ManagedCollections();
				for (var j = 0; j < length keys; j++) {
					var k = keys[j];
					if (k)
						res[k] = k;
				}
			}
			
		}
		return GetKeys(res);
	}

	CacheReset() {
		for (var i = 0; i < length managed_dbs; i++) {
			var db = managed_dbs[i];
			if (db)
				db.ResetCache();
		}
	}

	Execute(var data) {
		var key;
		var trigger;
		if (length data == 1) {
			var req_type = data[0];
			switch (req_type) {
				case -8:
					ResetDBS();
					return;
				case -9:
					ResetJS();
					return;
				case -10:
					ResetTriggers();
					return;
				case -100:
					jsctx = new [];
					this.CacheReset();
					echo "Garbage collector forced in worker #StorageWorker${workerid + 1}\n";
					if (CheckReachability())
						echo " => unreachable objects found\n";
					return;
				case -101:
					Worker::Set("..//memory//..", "SW${workerid + 1}" , BinarizeObject(QueryWorkerBase::WorkerMemory(["jscontexts" => length jsctx, "dbs" => this.ManagedCollections()])));
					return;
			}
		}
		var socket = data[0];
		if (socket == -102) {
			if (journal)
				journal.DeleteOps(data[1], data[2]);
			return;
		}
		var obj = data[1];
		var target_worker = data[2];
		var target_key = data[3];
		var js_ctx = data[4];
		var priority = 0;
		var r = new R();

		if ((js_ctx) && (socket <= 0) && (!target_worker) && (!target_key)) {
			JSCall(js_ctx, obj);
			return;
		}
		if (obj) {
			r.q = obj.q;
			if (obj.x)
				priority = obj.x["priority"];
		}
		if ((obj) && (obj.d)) {
			try {
				var db = EnsureDB(obj);
				if ((obj.c) && ((Pos(obj.c, "/") > 0) || (Pos(obj.c, "\\") > 0)))
					throw "Collection name error";

				var multi_insert = false;
				var use_sync_worker = syncworker;
				switch (obj.o) {
					case "Q":
						var t = db[obj.c];
						if (t) {
							if ((obj.f) && (typeof obj.f != "array"))
								obj.f = null;
							if (typeof obj.p != "array")
								obj.p = [ ];
							var map = false;
							var reduce = "";
							if (obj.x) {
								if (obj.x["map"])
									map = true;
								reduce = obj.x["reduce"];
								if (obj.x["explain"]) {
									r.id = 0;
									r.p = t.Explain(obj.p);
									break;
								}
							}
							t.Lock();
							if (reduce) {
								try {
									var js = EnsureJSContext(db);
									js.UserData[0]++;
									js.variable("out", new DummyObject());
									js.timeout(0);
									js.script(jstrycatch("var _r = function(self) { " + reduce + " }"));
									t.Query(obj.p, obj.f, obj.x["descending"], obj.x["start"], obj.x["len"], function(o, js) {
										if (o) {
											// js.variable("self", o);
											// js.script("_r();");
											js.timeout(REDUCE_TIMEOUT_S);
											js.call("_r", [o]);
										}
									}, js);
									if (js.LastError) {
										r.e = ERR_JS_SCRIPT;
										r.et = js.LastError;
									}
									js.timeout(0);
									r.p = js.script("out;");
									js.script("delete out;");
									if ((r.p) && (typeof r.p == "array"))
										QueryWorker::Cache(obj, r.p, obj.x["cache"]);
									js.gc();
								} catch (var js_error) {
									js.gc();
									// no need to reset here
									// ResetJSContext(db);
									r.e = ERR_JS_SCRIPT;
									r.et = js_error;
								}
								js.UserData[0]--;
							} else
							if ((!map) || (obj.worker)) {
								r.p = t.Query(obj.p, obj.f, obj.x["descending"], obj.x["start"], obj.x["len"]);
								QueryWorker::Cache(obj, r.p, obj.x["cache"]);
							} else
							if ((target_worker) && (target_key)) {
								r.p = null;
								t.Query(obj.p, obj.f, obj.x["descending"], obj.x["start"], obj.x["len"], function(o, data) {
									if (o) {
										var obj = data[0];
										var r = new R();
										r.q = obj.q;
										r.p = o;
										r.id = o['$'];
										r.e = NOTIFY_MORE_DATA;
										AddWorkerData(data[1], BinarizeObject([data[2], data[3], r]), data[4]);
									}
								}, [obj, target_worker, socket, target_key, priority]);
							} else {
								r.p = null;
								t.Query(obj.p, obj.f, obj.x["descending"], obj.x["start"], obj.x["len"], function(o, data) {
									if (o) {
										var obj = data[0];
										var r = new R();
										r.q = obj.q;
										r.p = o;
										r.id = o['$'];
										r.e = NOTIFY_MORE_DATA;
										r.Send(data[1], obj.io);
									}
								}, [obj, socket]);
							}
							t.Unlock();
							r.id = 0;
						}
						break;
					// Old delete, inefficient (memory)
					// case "D":
					// 	t = db[obj.c];
					// 	if (t) {
					// 		if ((obj.f) && (typeof obj.f != "array"))
					// 			obj.f = null;
					// 		if (typeof obj.p != "array")
					// 			obj.p = [ ];
					// 		t.Lock();
					// 		var items = t.Query(obj.p, obj.f, obj.x["descending"], obj.x["start"], obj.x["len"]);
					// 		t.Unlock();
					// 		if (items) {
					// 			t.Lock(true);
					// 			QueryWorker::InvalidateCache(obj.d, obj.c);
					// 			t.Delete(items);
					// 			t.Unlock(true);
					// 			non_flushed_writes += length items;
					// 			obj.p = items;
					// 			trigger = _triggers["${obj.d}.${obj.c}"];
					// 			if ((trigger) && (trigger.OnDelete)) {
					// 				var deleg = this.GetDelegate(db, obj.d, trigger.OnDelete, objects, members);
					// 				if (deleg) {
					// 					try {
					// 						r.p['$trigger'] = deleg(t, items);
					// 					} catch (var deleg_exception) {
					// 						echo "Uncaught delegate exception: $deleg_exception\n";
					// 					}
					// 				}
					// 			}
					// 		}
					// 		r.p['$count'] = length items;
					// 		// force flush/fsync for delete operations
					// 		t.FlushDB(false);
					// 	}
					// 	break;
					case "I":
						if ((obj.c) && (typeof obj.p == "array")) {
							t = db[obj.c];
							if (obj.x["multi"])
								multi_insert = true;
							var id = -1;
							var deleg;
							var deleg_exception;
							if (t) {
								var unique_fields = obj.x["unique"];
								trigger = _triggers["${obj.d}.${obj.c}"];
								if (obj.notrig)
									trigger = null;
								if ((TIN_CLUSTER) && (!obj.version))
									obj.version = this.oid();
								t.Reopen(true);
								t.Lock(true);
								// QueryWorker::InvalidateCache(obj.d, obj.c);
								var oid = null;
								if (multi_insert) {
									id = new [];
									t.SetIndexLock(2);
									for (var i = 0; i < length obj.p; i++) {
										var o = obj.p[i];
										if (typeof o == "array") {
											oid = null;
											if ((unique_fields) && (!t.IsUnique(o, unique_fields))) {
												r.e = ERR_NOT_UNIQUE;
												r.et = ERR_NOT_UNIQUE_S;
												break;
											}
											if ((TIN_CLUSTER || (IsSet(o, '$oid'))) && (!o['$oid'])) {
												oid = this.oid();
												o['$oid'] = oid;
											}

											if ((trigger) && (trigger.OnJSBeforeInsert)) {
												var trigger_error = JSTrigger(db, o, trigger.OnJSBeforeInsert, true);
												if (trigger_error) {
													r.e = ERR_TRIGGER;
													r.et = "OnBeforeInsert(JS): " + trigger_error;
													break;
												}
											}
											try {
												if (obj.version)
													o['$v'] = obj.version;

												var id2 = t.Store(o);
											} catch (var store_exc) {
												echo "Store error: $store_exc\n";
												r.e = ERR_INTERNAL;
												r.et = store_exc;
												break;
											}
											if ((use_sync_worker) && (TIN_FULLOBJECT) && (oid))
												QueryWorker::FullObjectSync(obj, o, oid, use_sync_worker);

											non_flushed_writes++;
											id[length id] = id2;
											o['$'] = id2;											
											if (trigger) {
												if (trigger.OnJSInsert) {
													trigger_error = JSTrigger(db, o, trigger.OnJSInsert);
													if (trigger_error) {
														r.e = "OnInsert(JS): " + ERR_TRIGGER;
														r.et = trigger_error;
														break;
													}
												}
												if (trigger.OnInsert) {
													deleg = this.GetDelegate(db, obj.d, trigger.OnInsert, objects, members);
													if (deleg) {
														try {
															deleg(t, o);
														} catch (deleg_exception) {
															echo "Uncaught delegate exception: $deleg_exception\n";
														}
													}
												}
											}
										}
									}
									t.SetIndexLock(false);
									r.id = id;
									if ((use_sync_worker) && (TIN_FULLOBJECT))
										use_sync_worker = null;
								} else {
									if ((unique_fields) && (!t.IsUnique(obj.p, unique_fields))) {
										r.e = ERR_NOT_UNIQUE;
										r.et = ERR_NOT_UNIQUE_S;
									} else
									if ((IsSet(obj.p, '$')) && (obj.p['$'] > 0)) {
										o = obj.p;
										id = value o['$'];
										if (obj.version)
											o['$v'] = obj.version;
											
										if ((trigger) && (trigger.OnJSBeforeUpdate)) {
											trigger_error = JSTrigger(db, o, trigger.OnJSBeforeUpdate, true);
											if (trigger_error) {
												r.e = ERR_TRIGGER;
												r.et = "OnBeforeUpdate(JS): " + trigger_error;
												break;
											}
										}
										if (!t.Update(obj.p, id, true, oid, use_sync_worker, use_sync_worker && TIN_FULLOBJECT, var syncobj)) {
											id = -1;
										} else
										if (use_sync_worker) {
											if (oid) {
												obj.o = "U";
												if (TIN_FULLOBJECT) {
													if (QueryWorker::FullObjectSync(obj, syncobj ?? obj.p, oid, use_sync_worker)) {
														use_sync_worker = null;
													} else
													if (syncobj) {
														obj.f = syncobj;
													} else {
														obj.f = obj.p;
														use_sync_worker = null;
													}
												} else
													obj.f = obj.p;
												obj.p = ['$oid' => oid];
											} else
												use_sync_worker = null;
										}
										non_flushed_writes++;
										if (trigger) {
											if (trigger.OnJSUpdate) {
												trigger_error = JSTrigger(db, o, trigger.OnJSUpdate);
												if (trigger_error) {
													r.e = "OnJSUpdate(JS): " + ERR_TRIGGER;
													r.et = trigger_error;
													break;
												}
											}
											if (trigger.OnUpdate) {
												deleg = this.GetDelegate(db, obj.d, trigger.OnUpdate, objects, members);
												if (deleg) {
													try {
														r.p['$trigger'] = deleg(t, obj.p);
													} catch (deleg_exception) {
														echo "Uncaught delegate exception: $deleg_exception\n";
													}
												}
											}
										}
									} else {
										if ((typeof obj.p == "array") && (TIN_CLUSTER || (IsSet(obj.p , '$oid'))) && (!obj.p['$oid'])) {
											oid = this.oid();
											obj.p['$oid'] = oid;
										}

										if ((trigger) && (trigger.OnJSBeforeInsert)) {
											trigger_error = JSTrigger(db, obj.p, trigger.OnJSBeforeInsert, true);
											if (trigger_error) {
												r.e = ERR_TRIGGER;
												r.et = "OnBeforeInsert(JS): " + trigger_error;
												break;
											}
										}
										if (obj.version)
											obj.p['$v'] = obj.version;
										id = t.Store(obj.p);
										if ((use_sync_worker) && (TIN_FULLOBJECT) && (oid)) {
											if (QueryWorker::FullObjectSync(obj, obj.p, oid, use_sync_worker))
												use_sync_worker = null;
										}
										non_flushed_writes++;
										if (trigger) {
											if (trigger.OnJSInsert) {
												obj.p['$'] = id;
												trigger_error = JSTrigger(db, o, trigger.OnJSInsert);
												if (trigger_error) {
													r.e = "OnInsert(JS): " + ERR_TRIGGER;
													r.et = trigger_error;
													break;
												}
											}
											if (trigger.OnInsert) {
												deleg = this.GetDelegate(db, obj.d, trigger.OnInsert, objects, members);
												if (deleg) {
													try {
														r.p['$trigger'] = deleg(t, obj.p);
													} catch (deleg_exception) {
														echo "Uncaught delegate exception: $deleg_exception\n";
													}
												}
											}
										}
									}
									r.id = id;
									if (id > 0)
										obj.p['$'] = id;
								}
								if (r.id)
									QueryWorker::InvalidateCacheSync(obj, use_sync_worker);

								if (TIN_FORCESYNC)
									t.FlushDB(false);
								else
									t.Flush();
								t.Unlock(true);
								// t.Flush();
							} else {
								r.e = ERR_IO;
								r.et = ERR_IO_S;
							}
						} else {
							r.e = ERR_COLLECTION;
							r.et = ERR_COLLECTION_S;
						}
						break;
					case "U":
						t = db[obj.c];
						if (t) {
							if (typeof obj.f == "array") {
								if (typeof obj.p != "array")
									obj.p = [ ];
								t.Reopen(true);
								t.Lock(true);
								r.p = null;
								unique_fields = obj.x["unique"];
								trigger = _triggers["${obj.d}.${obj.c}"];
								if (obj.notrig)
									trigger = null;
								deleg = null;
								if ((trigger) && (trigger.OnUpdate))
									deleg = this.GetDelegate(db, obj.d, trigger.OnUpdate, objects, members);

								reduce = obj.x["reduce"];
								js = null;
								if (reduce) {
									js = EnsureJSContext(db);
									js.UserData[0]++;
									js.timeout(0);
									js.script(jstrycatch("var _r = function(self) { " + reduce + "; return self; }"));
								}
								if ((TIN_CLUSTER) && (!obj.version))
									obj.version = this.oid();

								var update_data = [GetKeys(obj.f), obj, t, unique_fields, deleg, trigger, db, ((IsSet(obj.p, '$')) && (obj.p['$'])), js, obj.version];
								t.Query(obj.p, null, obj.x["descending"], obj.x["start"], obj.x["len"], function(o, data) {
									if (o) {
										var keys = data[0];
										var obj = data[1];
										var vals = obj.f;
										var t = data[2];
										var unique_fields = data[3];
										var deleg = data[4];
										var trigger = data[5];
										var db = data[6];
										var has_rowid = data[7];
										var js = data[8];
										var version_oid = data[9];
										var word_cache = data[10];
										var changed = new [];
										var oid_changed = false;
										var old_values = new [];
										var old_changed = new [];
										var sync_worker = this.syncworker;
										var i;
										if (obj.origp)
											data[16]++;
										if (js) {
											js.timeout(REDUCE_TIMEOUT_S);
											var self = js.call("_r", [o]);
											if (js.LastError)
												throw js.LastError;
											if ((self) && (typeof self == "array")) {
												var new_vals = new [];
												this.CopyKeys(self, new_vals);
												this.CopyKeys(vals, new_vals);
												keys = GetKeys(new_vals);
												vals = new_vals;
											}
										}
										for (i = 0; i < length keys; i++) {
											var k = keys[i];
											if ((k) && (k != '$')) {
												var val = vals[k];
												var o_k = o[k];
												if ((!val) && (k == '$oid')) {
													// do not update oid for objects that already have an oid
													if (o_k)
														continue;
													val = this.oid();
													oid_changed = true;
												} else
												if (typeof val == "array") {
													if (IsSet(val, "+"))
														val = o_k + val["+"];
													else
													if (IsSet(val, "-"))
														val = ForceNumber(o_k) - ForceNumber(val["-"]);
													else
													if (IsSet(val, "*"))
														val = ForceNumber(o_k) * ForceNumber(val["-"]);
													else
													if (IsSet(val, "/"))
														val = ForceNumber(o_k) / ForceNumber(val["/"], true);
													else
													if (IsSet(val, "%"))
														val = ForceNumber(o_k) % ForceNumber(val["%"], true);
													else
													if (IsSet(val, "=")) {
														val = ForceEval(o, val["="], o_k, k);
													}
												}

												if ((typeof o_k != typeof val) || (o_k != val)) {
													old_changed[k] = o_k;
													o[k] = val;
													changed[k] = val;
												}
												old_values[k] = o_k;
											}
										}
										if (changed) {
											var oid = o['$oid'];
											if (oid)
												oid_changed = true;

											if ((syncworker) && (has_rowid)) {
												data[13] = true;
												if (oid) {
													if (data[14]) {
														var arr = data[14];
														if (typeof arr == "array")
															arr[length arr] = oid;
														else
															data[14] = [arr, oid];
													} else
														data[14] = oid;
												}
											}

											if ((unique_fields) && (!t.IsUnique(o, unique_fields))) {
												// not unique
												data[12]++;
											} else {
												var filepos = t.Tell();
												try {
													if ((trigger) && (trigger.OnJSBeforeUpdate)) {
														var trigger_error = JSTrigger(db, o, trigger.OnJSBeforeUpdate, true);
														if (trigger_error)
															throw trigger_error;
														// set it to null, to force full reindex (before update may modify the object)
														changed = null;
													}
													if (data[11] == 1) {
														// at least one record
														word_cache = t.BM25UpdateStart();
														data[10] = word_cache;
													}
													if (version_oid)
														o['$v'] = version_oid;
													t._StoreHint(o, true, true, oid_changed, changed, word_cache, old_values, old_changed);

													if ((sync_worker) && (TIN_FULLOBJECT) && (oid) && (!obj.origp)) {
														// disable classic sync
														if (QueryWorker::FullObjectSync(obj, o, oid, sync_worker))
															data[15] = true;
													}

													if ((trigger) && (trigger.OnJSUpdate)) {
														trigger_error = JSTrigger(db, o, trigger.OnJSUpdate);
														if (trigger_error)
															throw trigger_error;
													}
													data[11]++;

													if (deleg)
														deleg(t, o);
												} catch (var exc) {
													echo "Error in update: $exc\n";
												}
												t.Seek(filepos);
											}
										}
									}
								}, update_data);
								if (update_data[10])
									t.BM25UpdateEnd(update_data[10]);
								if ((use_sync_worker) && (obj.origp) && (TIN_FULLOBJECT)) {
									if ((!update_data[16]) && (obj.f)) {
										obj.f['$'] = 0;
										t.Store(obj.f);
										QueryWorker::InvalidateCacheSync(obj, use_sync_worker);
										non_flushed_writes ++;
									} else
									if (update_data[11]) {
										QueryWorker::InvalidateCacheSync(obj, use_sync_worker);
										non_flushed_writes += update_data[11];
									}
								} else
								if (update_data[11]) {
									// QueryWorker::InvalidateCache(obj.d, obj.c);
									if (use_sync_worker) {
										if (update_data[15]) {
											use_sync_worker = null;
										} else
										if (update_data[13]) {
											oid = update_data[14];
											if (oid)
												obj.p = ['$oid' => oid];
											else
												use_sync_worker = null;
										}
									}
									QueryWorker::InvalidateCacheSync(obj, use_sync_worker);
									non_flushed_writes += update_data[11];
								}
								if (unique_fields)
									r.p = ["\$updated" => update_data[11], "\$notunique" => update_data[12]];
								else
									r.p = ["\$updated" => update_data[11]];
								t.Unlock(true);
								if (js)
									js.gc();
							}
						}
						r.id = 0;
						break;
					case "D":
						t = db[obj.c];
						if (t) {
							if (typeof obj.p != "array")
								obj.p = [ ];
							t.Reopen(true);
							t.Lock(true);
							r.p = null;
							unique_fields = obj.x["unique"];
							trigger = _triggers["${obj.d}.${obj.c}"];
							if (obj.notrig)
								trigger = null;
							deleg = null;
							if ((trigger) && (trigger.OnDelete))
								deleg = this.GetDelegate(db, obj.d, trigger.OnDelete, objects, members);

							update_data = [deleg, t, trigger, db, ((IsSet(obj.p, '$')) && (obj.p['$']))];
							t.Query(obj.p, null, obj.x["descending"], obj.x["start"], obj.x["len"], function(o, data) {
								if (o) {
									var deleg = data[0];
									var t = data[1];
									var trigger = data[2];
									var db = data[3];
									var filepos = t.Tell();
									try {
										if ((trigger) && (trigger.OnJSBeforeDelete)) {
											var trigger_error = JSTrigger(db, o, trigger.OnJSBeforeDelete, true);
											if (trigger_error)
												throw trigger_error;
										}
										t.Delete(o['$']);
										if ((syncworker) && (data[4])) {
											data[5] = true;
											var oid = o['$oid'];
											if (oid) {
												if (data[6]) {
													var arr = data[6];
													if (typeof arr == "array")
														arr[length arr] = oid;
													else
														data[6] = [arr, oid];
												} else
													data[6] = oid;
											}
										}

										data[7]++;
										if ((trigger) && (trigger.OnJSDelete)) {
											trigger_error = JSTrigger(db, o, trigger.OnJSDelete);
											if (trigger_error)
												throw trigger_error;
										}
										if (deleg)
											deleg(t, o);
									} catch (var exc) {
										echo "Error in delete: $exc\n";
									}
									t.Seek(filepos);
								}
							}, update_data);
							var count = update_data[7];
							if (count) {
								if ((use_sync_worker) && (update_data[5])) {
									oid = update_data[6];
									if (oid)
										obj.p = ['$oid' => oid];
									else
										use_sync_worker = null;
								}								
								QueryWorker::InvalidateCacheSync(obj, use_sync_worker);
								non_flushed_writes += count;
							}
							t.Unlock(true);
							r.p = ['$count' => count];
							if (count) {
								this.Flush();
								this.DeleteFromJournal(obj);
							}
						}
						r.id = 0;
						break;
					case "X":
						r.id = 0;
						t = db[obj.c];
						if ((obj.p) && (typeof obj.p == "array")) {
							t.Lock();
							var start = microseconds();
							var created_indexes = t.EnsureIndexes(obj.p);
							var end = microseconds();
							t.Unlock();
							if (created_indexes) {
								r.p["created"] = true;
								r.p["duration"] = (end - start) / 1000;
								// QueryWorker::InvalidateCache(obj.d, obj.c);
								QueryWorker::InvalidateCacheSync(obj, syncworker);
								this.ResetDBS(true);
							} else {
								r.p["created"] = false;
								r.p["exists"] = true;
							}
							non_flushed_writes++;
						}
						break;
					case "Z":
						r.id = 0;
						t = db[obj.c];
						if ((obj.p) && (typeof obj.p == "array")) {
							t.Lock(true);
							if (obj.p["bm25"])
								var removed = t.DropBM25Index();
							else
								removed = t.DeleteIndexes(obj.p);
							t.Unlock(true);
							if (removed) {
								r.p["removed"] = true;
								r.p["exists"] = true;
								// QueryWorker::InvalidateCache(obj.d, obj.c);
								QueryWorker::InvalidateCacheSync(obj, syncworker);
								this.ResetDBS(true);
								non_flushed_writes++;
							} else
								r.p["exists"] = false;
						}
						break;
					case "Y":
						r.id = 0;
						t = db[obj.c];
						if (typeof obj.p == "array") {
							var lang = "en";
							if ((obj.x) && (obj.x["lang"]))
								lang = obj.x["lang"];
							echo "Full text index requested (building index) ...\n";
							var start_time = microseconds();
							t.Lock();
							var index_exists = t.EnsureFullTextIndex(obj.p, lang);
							t.Unlock();
							var end_time = microseconds() - start_time;
							if (index_exists) {
								r.p["created"] = false;
								r.p["exists"] = true;
								echo "Index exists (${end_time/1000}ms)\n";
							} else {
								r.p["created"] = true;
								r.p["duration"] = end_time/1000;
								echo "Full text index created in ${end_time/1000}ms\n";
								// QueryWorker::InvalidateCache(obj.d, obj.c);
								QueryWorker::InvalidateCacheSync(obj, syncworker);
								this.ResetDBS(true);
							}
							non_flushed_writes++;
						}
						break;
					case "S":
						t = db[obj.c];
						if (t) {
							if ((obj.f) && (typeof obj.f != "array"))
								obj.f = null;
							var searchstring = "";
							if (typeof obj.p == "array")
								searchstring = obj.p[0];
							r.p = t.Search(searchstring, obj.f, obj.x["start"], obj.x["len"]);
							r.id = 0;
						}
						break;
					case "H":
						var remove = obj.p["remove"];
						var code = obj.p["code"];
						if (obj.p["js"]) {
							js = ResetJSContext(db, true);
							js.UserData[0]++;
							try {
								js.timeout(0);
								js.script(jstrycatch(code));
								ResetJS(true);
								db.StoreJS(code);
								js.UserData[0]--;
							} catch (var js_exc) {
								js.UserData[0]--;
								ResetJSContext(db);
								r.e = ERR_JS_SCRIPT;
								r.et = js_exc;
							}
						} else {
							if (remove) {
								if (typeof remove != "array") {
									RemoveClass(remove);
								} else {
									for (i = 0; i < length remove; i++)
										RemoveClass(remove[i]);
								}
								objects = new [];
								members = new [];
							}
							if (code) {
								var err = IncludeCode(code);
								if (err) {
									r.e = ERR_IN_CODE;
									r.et = err;
								} else {
									db.StoreHandler(code);
								}
							}
						}
						break;
					case ".":
						code = obj.p["code"];
						var member = null;
						if (code) {
							if (obj.p["js"]) {
								try {
									var js_parameters = obj.p["params"];
									js = EnsureJSContext(db);
									if (typeof js_parameters == "array") {
										var js_keys = GetKeys(js_parameters);
										for (var i2 = 0; i2 < length js_keys; i2++) {
											var js_key = js_keys[i2];
											if (js_key)
												js.variable(js_key, js_parameters[js_key]);
										}
									}
									js.UserData[0]++;
									js.timeout(0);
									r.p = js.script(jstrycatch(code));
									js.gc();
									js.UserData[0]--;
								} catch (js_error) {
									js.UserData[0]--;
									ResetJSContext(db);
									r.e = ERR_JS_SCRIPT;
									r.et = js_error;
								}
							} else {
								var arr = StrSplit(code, ".");
								if (length arr == 2) {
									var classname = arr[0];
									var mname = arr[1];
									key = "${obj.d}.$classname";
									var key_mname = "$key.$mname";
									member = members[key_mname];
									if (!member) {
										var handler = objects[key];
										if (!handler) {
											handler = CreateObject2(classname, db);
											if (handler)
												objects[key] = handler;
										}
										if (handler) {
											if (!GetMember(handler, mname, member, true))
												member = null;
										}
									}
									if (member) {
										var parameters = obj.p["params"];
										try {
											if (parameters)
												r.p = member(parameters);
											else
												r.p = member();
										} catch (deleg_exception) {
											echo "Uncaught delegate exception: $deleg_exception\n";
										}
									} else {
										r.e = ERR_IN_CODE;
										r.et = ERR_NO_MEMBER_S;
									}
								}
							}
						}
						break;
					case "*":
						if ((obj.c) && (typeof obj.p == "array")) {
							key = "${obj.d}.${obj.c}";
							trigger = _triggers[key];
							if (!trigger) {
								trigger = new Triggers();
								_triggers[key] = trigger;
							}
							// insert
							// update
							// delete
							if (obj.p["js"]) {
								if (obj.p["before"]) {
									if (IsSet(obj.p, "insert"))
										trigger.OnJSBeforeInsert = obj.p["insert"];

									if (IsSet(obj.p, "update"))
										trigger.OnJSBeforeUpdate = obj.p["update"];

									if (IsSet(obj.p, "delete"))
										trigger.OnJSBeforeDelete = obj.p["delete"];
								} else {
									if (IsSet(obj.p, "insert"))
										trigger.OnJSInsert = obj.p["insert"];

									if (IsSet(obj.p, "update"))
										trigger.OnJSUpdate = obj.p["update"];

									if (IsSet(obj.p, "delete"))
										trigger.OnJSDelete = obj.p["delete"];
								}
							} else {
								if (IsSet(obj.p, "insert")) {
									trigger.OnInsert = obj.p["insert"];
									if (!this.GetDelegate(db, obj.d, trigger.OnInsert, objects, members, r)) {
										trigger.OnInsert = null;
										break;
									}
								}
								if (IsSet(obj.p, "update")) {
									trigger.OnUpdate = obj.p["update"];
									if (!this.GetDelegate(db, obj.d, trigger.OnUpdate, objects, members, r)) {
										trigger.OnUpdate = null;
										break;
									}
								}
								if (IsSet(obj.p, "delete")) {
									trigger.OnDelete = obj.p["delete"];
									if (!this.GetDelegate(db, obj.d, trigger.OnDelete, objects, members, r)) {
										trigger.OnDelete = null;
										break;
									}
								}
							}
							db.StoreTriggers(_triggers);
							ResetTriggers(true);
						} else {
							r.e = ERR_COLLECTION;
							r.et = ERR_COLLECTION_S;
						}
						break;
					case "R":
						if (obj.c) {
							t = db[obj.c];
							if (t) {
								try {
									t.Drop();
									RemoveCollectionFS(obj.d, obj.c);
									db.Reset();
								} catch (var exc_drop) {
									echo "Remove error: $exc_drop\n";
									r.e = ERR_COLLECTION;
									r.et = ERR_COLLECTION_S;
								}
								// QueryWorker::InvalidateCache(obj.d, obj.c);
								QueryWorker::InvalidateCacheSync(obj, syncworker);
								this.ResetDBS(true);
								this.DeleteFromJournal(obj);
							}
						} else {
							r.e = ERR_COLLECTION;
							r.et = ERR_COLLECTION_S;
						}
						break;
					case "L":
						if ((obj.c) && (typeof obj.p == "array")) {
							t = db[obj.c];
							var hash = "sha256";
							var username = "username";
							var field = "password";
							var challenge = "";
							var sum = "";
							if (obj.x["f"])
								hash = ToLower("" + obj.x["f"]);

							if (obj.x["pass_field"])
								field = obj.x["pass_field"];

							if (obj.x["user_field"])
								username = obj.x["user_field"];

							if (obj.x["challenge"])
								challenge = obj.x["challenge"];

							if (obj.x["hash"])
								sum = obj.x["hash"];

							if ((t) && (field) && (username)) {
								if (obj.f) {
									obj.f[username] = username;
									obj.f[field] = field;
								}
								t.Lock();
								var user_arr = t.Query(obj.p, obj.f, false, 0, 1);
								t.Unlock();
								var user = null;
								if (user_arr)
									user = user_arr[0];
								if (user) {
									var d2 = "";
									switch (hash) {
										case "sha256":
											d2 = sha256(user[username] + ":" + challenge + ":" + user[field]);
											break;
										case "sha1":
											d2 = sha1(user[username] + ":" + challenge + ":" + user[field]);
											break;
										case "":
											d2 = user[field];
											break;
									}
									if (d2 == sum) {
										user[field] = "";
										r.p = user;
									} else {
										r.e = ERR_USER_PASS;
										r.et = ERR_USER_PASS_S;
									}
								} else {
									r.e = ERR_USER_PASS;
									r.et = ERR_USER_PASS_S;
								}
							} else {
								r.e = ERR_IO;
								r.et = ERR_IO_S;
							}
						} else {
							r.e = ERR_COLLECTION;
							r.et = ERR_COLLECTION_S;
						}
						break;
					case "F":
						t = db[obj.c];
						if ((t) && (obj.p)) {
							var offset = obj.p["o"];
							var name = obj.p["n"];
							if (typeof name != "string")
								name = "";
							if (name) {
								name = StrReplace(name, "/", "_");
								name = StrReplace(name, "\\", "_");
								name = StrReplace(name, "..", "_");
							}
							if (name) {
								var stored_bytes = 0;
								var file_buffer = obj.p["b"];
								if (obj.p["delete"]) {
									t.Lock(true);
									var deleted = DeleteFile(obj.d, obj.c, name);
									r.p = ["deleted" => deleted, "name" => name];
									t.Unlock();
								} else {
									if ((file_buffer) && (typeof file_buffer == "string")) {
										t.Lock(true);
										stored_bytes = StoreFile(obj.d, obj.c, name, offset, file_buffer);
										t.Unlock();
									}
									r.p = ["bytes" => stored_bytes, "name" => name];
								}
							} else {
								r.e = ERR_FILENAME;
								r.et = ERR_FILENAME_S;
							}
						} else {
							r.e = ERR_COLLECTION;
							r.et = ERR_COLLECTION_S;
						}
						break;
					case "B":
						t = db[obj.c];
						if ((t) && (obj.p)) {
							offset = obj.p["o"];
							var size = obj.p["s"];
							name = obj.p["n"];
							if (typeof name != "string")
								name = "";
							if (name) {
								name = StrReplace(name, "/", "_");
								name = StrReplace(name, "\\", "_");
								name = StrReplace(name, "..", "_");
							}
							if (name) {
								t.Lock();
								r.p = GetFileBuffer(obj.d, obj.c, name, offset, size);
								t.Unlock();
							} else {
								r.e = ERR_FILENAME;
								r.et = ERR_FILENAME_S;
							}
						} else {
							r.e = ERR_COLLECTION;
							r.et = ERR_COLLECTION_S;
						}
						break;
				}
			} catch (var exc) {
				if (t)
					t.ReleaseLock();
				r.e = ERR_COLLECTION;
				r.et = exc;
			}
		} else {
			r.e = ERR_DB;
			r.et = ERR_DB_S;
		}
		if ((!obj) || (!obj.x) || (!IsSet(obj.x, 'silent')) || (!obj.x['silent'])) {
			if (obj.worker) {
				obj.r = r;
				AddWorkerData(obj.worker, BinarizeObject([socket, obj, 0, target_key]), priority);
			} else
			if ((target_worker) && (target_key)) {
				AddWorkerData(target_worker, BinarizeObject([socket, target_key, r]), priority);
			} else
			if ((socket < 0) && (!obj.io)) {
				if (obj.cb)
					AddWorkerData(obj.cb["io"], BinarizeObject([-1, r, 0, "", obj.cb]), priority);
				else
					return r;
			} else
				r.Send(socket, obj.io);
		}
	}

	oid() {
		if (counter >= 0xFFFF)
			counter = 0;
		return TinDBOid::create(machineid, workerid, counter++);
	}

	StorageWorker(n) {
		if (TIN_JOURNALING)
			journal = new Journal(n);

		Worker::Pending(var storage_data, -1);
		var n_data = UnBinarizeObject(storage_data);
		var all_workers = n_data[0];
		jsworker = n_data[1];
		syncworker = n_data[2];
		machineid = n_data[3];
		var current_worker = CurrentWorker();
		counter = RandomInteger(0, 0xFFF0);

		if (all_workers) {
			for (var j = 0; j < length all_workers; j++) {
				var worker = all_workers[j];
				if ((worker) && (worker != current_worker))
					other_workers[length other_workers] = worker;
				else
				if (worker)
					workerid = j;

			}
		}

		var data;
		while (true) {
			data = null;
			var qsize = Worker::PendingAll(data, -1, 100);
			if (data) {
				var len = length data;
				for (var i = 0; i < length data; i++) {
					try {
						Execute(UnBinarizeObject(data[i]));
					} catch (var exc) {
						echo "Storage worker exception: $exc\n";
					}
					if ((non_flushed_writes) && ((non_flushed_writes > BUSY_FSYNC_LIMIT) || ((time() - last_flush) >= BUSY_FSYNC_TIMEOUT)))
						this.Flush();
				}
				if ((non_flushed_writes) && (qsize == len))
					this.Flush();
			}
		}
	}
}

class QueryAggregator {
	var QueryWorkers;
	var QueryWorkerIndex = 0;

	PopulateValues(arr, key) {
		var[] values;
		if ((arr) && (key)) {
			var[] keys;
			for (var i = 0; i < length arr; i++) {
				var e = arr[i];
				if ((e) && (IsSet(e, key))) {
					var v = e[key];
					if (typeof v == "string")
						keys[v] = v;
					else
					if (typeof v == "numeric") {
						v = "" + v;
						keys[v] = v;
					}
				}
			}
			for (i = 0; i < length keys; i++)
				values[i] = keys[i];
		}	
		return values;
	}

	Pairs(elements, pairs) {
		var[] pairs_tree;
		var keys = GetKeys(pairs);
		var[] tree_counts;
		for (var i = 0; i < length elements; i++) {
			var e = elements[i];
			if (e) {
				for (var j = 0; j < length keys; j++) {
					var k = keys[j];
					if (k) {
						var sorted_pairs = pairs_tree[k];
						var counts;
						if (sorted_pairs) {
							counts = tree_counts[k];
						} else {
							sorted_pairs = new [];
							pairs_tree[k] = sorted_pairs;
							counts = new [];
							tree_counts[k] = counts;
						}
						var equiv = pairs[k];
						if ((equiv) && (IsSet(e, equiv))) {
							var v = e[equiv];
							if ((typeof v == "string") || (typeof v == "numeric")) {
								v = "" + v;
								var count = counts[v];
								if (!count) {
									sorted_pairs[v] = e;
								} else
								if (count == 1) {
									sorted_pairs[v] = [sorted_pairs[v], e];
								} else {
									var arr = sorted_pairs[v];
									var key = e['$'];
									if (key)
										arr[key] = e;
									else
										arr[length arr] = e;
								}
								count++;
								counts[v] = count;
							}
						}
					}
				}
			}
		}
		return pairs_tree;
	}

	Filter(e, pairs, keys) {
		for (var i = 0; i < length keys; i++) {
			var k = keys[i];
			if ((k) && (IsSet(e, k))) {
				var v = e[k];	
				if ((typeof v == "string") || (typeof v == "numeric")) {
					v = "" + v;
					var arr = pairs[k];
					if (IsSet(arr, v)) {
						var tmp_key = '$' + k;
						while (e[tmp_key])
							tmp_key = '$' + tmp_key;
						e[tmp_key] = arr[v];
					}
				}
			}
		}
	}

	Execute(var data, recursive_count = 0) {
		var q = UnBinarizeObject(data);
		var obj;
		if (q)
			obj = q[1];
		var priority = 0;
		if (obj) {
			if (obj.x)
				priority = obj.x["priority"];

			var worker = QueryWorkers[QueryWorkerIndex++];
			if (QueryWorkerIndex >= length QueryWorkers)
				QueryWorkerIndex = 0;
			var data_ready = false;
			if (typeof obj.p == "array") {
				var new_obj = new Q;
				new_obj.o = "Q";
				new_obj.q = obj.q;
				new_obj.d = obj.d;
				new_obj.c = obj.c;
				new_obj.f = obj.f;

				new_obj.socket = obj.socket;
				new_obj.io = obj.io;
				new_obj.cb = obj.cb;
				new_obj.worker = CurrentWorker();
				new_obj.ctx_key = obj.ctx_key;
				if (obj.o == "W") {
					new_obj.origp = obj.p;
					new_obj.aindex = 0;
					new_obj.p = obj.p[0];
					new_obj.x = obj.x;
				} else
				if ((obj.r) && (obj.origp)) {
					var p;
					if (obj.child) {
						p = obj.child;
						new_obj.aindex = obj.aindex;
					} else {
						new_obj.aindex = obj.aindex + 1;
						p = obj.origp[new_obj.aindex];
					}

					var qdata;
					var ref_data;
					if (!obj.aindex)
						qdata = obj.r.p;
					else
					if ((obj.data) || (obj.child_data)) {
						qdata = obj.data[0];

						if (obj.child_data) {
							ref_data = obj.child_data[0];
							var pairs = Pairs(obj.r.p, obj.child_data[1]);
 						} else {
							ref_data = qdata;
							pairs = Pairs(obj.r.p, obj.data[1]);
						}

						if ((ref_data) && (pairs)) {
							var pair_keys = GetKeys(pairs);
							for (var i = 0; i < length ref_data; i++) {
								var e = ref_data[i];
								if (e)
									Filter(e, pairs, pair_keys);
							}
						}
					} else
						qdata = new [];

					if (p) {
						var keys = GetKeys(p);
						var k1 = keys[0];
						var k2 = keys[1];
						var has_query = true;
						var translate;
						var child_query = null;
						if ((length p == 3) && (typeof p[2] == "array"))
							child_query = p[2];
						if ((child_query) && (obj.r.p))
							new_obj.child = child_query;

						if ((!k1) && (!k2) && (length p >= 2) && (p[0]) && (typeof p[0] == "string")) {
							k1 = p[0];
							translate = p[1];
							has_query = false;
						}

						if (((length p != 2) && (length p != 3)) || ((!k1) && (!k2)) || ((k1 != '$') && (k2 != '$'))) {
							obj.r = new R();
							obj.r.q = obj.q;
							obj.r.e = ERR_AGGREGATION;
							obj.r.et = ERR_AGGREGATION_S;
							data_ready = true;
						} else {
							if (k1 == '$') {
								new_obj.c = k2;
								if (has_query) {
									new_obj.p = p[1];
									translate = p[0];
								}
							} else {
								new_obj.c = k1;
								if (has_query) {
									new_obj.p = p[0];
									translate = p[1];
								}
							}
							if (!new_obj.p)
								new_obj.p = new [];
							var translate_clean = new [];
							if (obj.child)
								ref_data = obj.r.p;
							else
								ref_data = qdata;
							if (translate) {
								keys = GetKeys(translate);
								for (i = 0; i < length keys; i++) {
									var k = keys[i];
									if (k) {
										var equivalent_k = translate[equivalent_k];
										if (equivalent_k) {
											var v = PopulateValues(ref_data, k);
											if (v) {
												new_obj.p[equivalent_k] = v;
												translate_clean[k] = equivalent_k;
											}
										}
									}
								}
							}
							if (qdata) {
								new_obj.origp = obj.origp;
								new_obj.data = [qdata, translate_clean];
								if (obj.child)
									new_obj.child_data = [obj.r.p, translate_clean];
							} else
								data_ready = true;
						}
					} else
						data_ready = true;
				} else
					data_ready = true;

				if (!data_ready)
					AddWorkerData(worker, BinarizeObject([-1, new_obj]), priority);
			} else
				data_ready = true;

			if (data_ready) {
				if (!obj.r) {
					obj.r = new R();
					obj.q = obj.q;
				}
				if (obj.data)
					obj.r.p = obj.data[0];
				if (obj.cb)
					AddWorkerData(obj.cb["io"], BinarizeObject([-1, obj.r, 0, "", obj.cb]), priority);
				else
					obj.r.Send(obj.socket, obj.io);
			}
		}
	}

	QueryAggregator(n) {
		n = UnBinarizeObject(n);
		QueryWorkers = n[1];
		var data = "";
		while (true) {
			if (Worker::PendingAll(data, -1)) {
				try {
					if (data) {
						for (var i = 0; i < length data; i++)
							Execute(data[i]);
					}
				} catch (var exc) {
					echo "Storage worker exception: $exc\n";
				}
				data = null;
			}
		}
	}
}

class QuerySyncWorker {
	protected var QueryWorkers;
	protected var QueryWorkerIndex = 0;
	protected var journal;
	protected var localjournal;
	protected var opid;
	protected var[] NotSynced;
	protected var[] NotWritten;
	protected var[] NotConfirmed;
	protected var[] Neighbours;
	protected var[] buffers;
	protected var[] local_ring;
	protected var[] validips;
	protected var[] verticalnodes;
	protected var NodeName = "";
	protected var NodeHost = "";
	protected var poll;
	protected var RingTimeout;
	protected var current_worker;
	protected var ring_checksum = "";
	protected var node_checksum = "";
	protected var ServerSocket;

	Log(str) {
		var now = trim(StrReplace(ctime(time()), "\r", ""));
		echo "$now: $str\n";
	}

	RemoveOffsetKey(p, var has_key_offset) {
		var keys = GetKeys(p);
		var new_p = new [];
		has_key_offset = false;
		for (var i = 0; i < length keys; i++) {
			var k = keys[i];	
			if (k) {
				if (k == '$')
					has_key_offset = true;
				else
					new_p[k] = p[k];
			}
		}
		return new_p;
	}

	Execute(var data) {
		var wait_confirmation = false;
		if (!data)
			return wait_confirmation;

		var obj = UnBinarizeObject(data);
		if (obj) {
			var origin_node = obj.origp;
			if (origin_node) {
				// already written
				if (origin_node == NodeName)
					return;
			} else
				origin_node = NodeName;
			var worker = QueryWorkers[QueryWorkerIndex++];
			if (QueryWorkerIndex >= length QueryWorkers)
				QueryWorkerIndex = 0;
			obj.MakeSafe();
			// don't use offsets
			if ((typeof obj.p == "array") && (IsSet(obj.p, '$'))) {
				obj.p = RemoveOffsetKey(obj.p, var has_key_offset);
				if ((has_key_offset) && (!obj.p)) {
					this.Log("Broken query received (has key offset, but no other parameters)");
					return;
				}
			}
			obj.notrig = true;
			obj.origp = origin_node;
			if (obj.r)
				obj.r.p = null;
			switch (obj.o) {
				case "I":
				case "U":
				case "D":
					// insert/update/delete neeed confirmation
					obj.io = current_worker;
					// obj.aindex = Key(obj);
					obj.q = "QSHA:" + sha1(data);
					if ((typeof obj.x == "array") && (IsSet(obj.x, "silent")))
						obj.x["silent"] = false;
					wait_confirmation = true;
					break;
			}
			if (TIN_CLUSTER_LOG)
				this.Log("Added '${obj.o}' to queue");
			AddWorkerData(worker, BinarizeObject([-20, obj]));
		}
		return wait_confirmation;
	}

	CreateNeighbour() {
		for (var i = 0; i < length local_ring; i++) {
			var node = local_ring[i];
			if (node) {
				var socket = new TCPSocket();
				try {
					if (socket.Connect(node.host, node.port)) {
						if (TIN_CLUSTER_LOG)
							this.Log("Connected to node ${node.host}:${node.port}");
						var sockfd = socket.Socket;	
						// no autoclose
						socket.Socket = -1;
						var ctx = this.AddSocket(sockfd, node.host, 1);
						if (ctx)
							return sockfd;
					} else
						echo "Cannot connect to node ${node.host}:${node.port}\n";
				} catch (var exc) {
					echo "Error connecting to ${node.host}:${node.port}: $exc\n";
				}
				// try just next neighbour to mantain correct order
				break;
			}
		}
		return null;
	}

	ChooseNeighbour() {
		for (var i = 0; i < length buffers; i++) {
			var ctx = buffers[i];
			if ((ctx) && (ctx.direction))
				return ctx.socket;
		}
		return CreateNeighbour();
	}

	SocketReadTimeout(socket, var buffer, len, timeout = SYNC_CONFIRM_TIMEOUT) {
		var iterations = SYNC_CONFIRM_TIMEOUT / 20;
		do {
			if (SocketHasData(socket, 20))
				return SocketRead(socket, buffer, len);
			this.PollSockets();
			iterations--;
		} while (iterations > 0);
		return -2;
	}

	Sync() {
		if (NotSynced) {
			var socket = ChooseNeighbour();
			if (socket) {
				var new_notsync = new [];
				for (var i = 0; i < length NotSynced; i++) {
					var data = NotSynced[i];
					if (data) {
						if ((socket) && (SocketIOBase::SendBufferWithSize(socket, data))) {
							var read_code = SocketReadTimeout(socket, var buffer, 3);
							if ((read_code != 3) || (buffer != "\x02OK")) {
								if (read_code == -2)
									echo "Node sync timed out, resetting connection ...\n";
								else
									echo "Node sync error(errno: ${SocketErrno()}), resetting connection ...\n";
								new_notsync[length new_notsync] = data;
								this.RemoveSocket(socket);
								socket = 0;
							} else
								journal.Log(data, "sync", opid++, 3);
						} else {
							new_notsync[length new_notsync] = data;
							// conection broken, remove neighbour
							this.RemoveSocket(socket);
						}
					}
				}
				NotSynced = new_notsync;
				// nothing to sync, reset journal
				if (length new_notsync == 0)
					journal.Reset();
			}
		}
	}

	NotifyNeighbours(var data) {
		if (data) {
			var data2 = compress(data);
			journal.Log(data2, "sync", opid++, 1);
			NotSynced[length NotSynced] = data2;
			Sync();
		}
	}

	Order(arr, index) {
		var[] nodes;
		for (var i = index + 1; i < length arr; i++)
			nodes[length nodes] = arr[i];

		for (i = 0; i < index; i++)
			nodes[length nodes] = arr[i];

		return nodes;
	}

	ClearOutNeighbours() {
		var[] neighbours;
		for (var i = 0; i < length buffers; i++) {
			var ctx = buffers[i];
			if ((ctx) && (ctx.direction))
				neighbours[length neighbours] = ctx;
		}
		for (i = 0; i < length neighbours; i++)
			this.RemoveSocket(neighbours[i].socket);
	}

	LoadRing(path = ".") {
		// set timeout here, in case we fail
		RingTimeout = time() + RING_RELOAD_TIMEOUT;

		var node = ReadFile(path + "/node.json");
		if (!node)
			return;

		var local_checksum = sha256(node);
		var data = ReadFile(path + "/ring.json");
		var data_checksum = sha256(data);

		if ((ring_checksum == data_checksum) && (node_checksum == local_checksum)) {
			// nothing changed;
			return;
		}

		node = JSONDeserialize(node);
		if (!node) {
			echo "Error parsing node config file\n";
			return;
		}

		var name = node["name"] ?? "";
		var host = node["host"] ?? "";
		var vertical = node["vertical"];
		var v_servers = null;
		var v_allow = null;
		if (typeof vertical == "string") {
			v_servers = [vertical];
			v_allow = v_servers;
		} else
		if ((vertical) && (typeof vertical == "array")) {
			v_servers = vertical["nodes"];
			v_allow = vertical["allow"];
		}

		if (!host) {
			echo "Node host is not set\n";
			return;
		}

		if (!data)
			return;

		data = JSONDeserialize(data);
		if ((!data) || (typeof data != "array")) {
			echo "Error paring ring config file\n";
			return;
		}

		NodeName = name ?? host;
		NodeHost = host;

		var keys = GetKeys(data);
		var[] all_nodes;
		var this_node_index = -1;
		var[] all_hosts;
		for (var i = 0; i < length keys; i++) {
			var node_element = data[i];
			if (node_element) {
				if (typeof node_element == "array") {
					var node_host = node_element["host"] ?? "";
					var node_name = node_element["name"] ?? node_host;
					var node_port = (value node_element["port"]) ?? 2669;
					if (node_host) {
						all_hosts[node_host] = true;
						if (node_host == host)
							this_node_index = length all_nodes;
						all_nodes["$node_host:$node_port"] = new SyncNode(node_host, node_name, node_port);
					}
				} else
				if (typeof node_element == "string") {
					node_host = node_element;
					if (node_host == host)
						this_node_index = length all_nodes;
					all_nodes["$node_host:2669"] = new SyncNode(node_host, node_host);
					all_hosts[node_host] = true;
				}
			}
		}
		if (v_allow) {
			for (i = 0; i < length v_allow; i++) {
				var v_host = v_allow[i];
				if ((v_host) && (typeof v_host == "string"))
					all_hosts[v_host] = true;
			}
		}

		validips = all_hosts;
		if (this_node_index == -1) {
			echo "Cannot identify current node in node list\n";
			return;
		}

		local_ring = Order(all_nodes, this_node_index);
		ClearOutNeighbours();
		// reset timeout (didn't fail)
		node_checksum = local_checksum;
		ring_checksum = data_checksum;
		RingTimeout = time() + RING_RELOAD_TIMEOUT;
		this.verticalnodes = v_servers;
	}

	ValidIP(ip) {
		if (IsSet(validips, ip))
			return validips[ip];
		return false;
	}

	RemoveSocket(sock) {
		var list = this.Neighbours;
		var new_list = new [];
		var buffers = this.buffers;
		var buf = new [];
		var keys = GetKeys(buffers);
		var sock_key = "" + sock;
		for (var i = 0; i < length list; i++) {
			var s = list[i];
			var k = keys[i];
			if (k) {
				var buf_k = buffers[k];
				if ((buf_k) && (s) && (s != sock) && (k != sock_key)) {
					new_list["" + buf_k.direction + "/" + buf_k.key] = s;
					buf[k] = buf_k;
				}
			}
		}
		var info = SocketInfo(sock);
		if (info) {
			var ip = info["address"];
			var port = info["port"];
			echo "Closing socket $ip:$port\n";
		}
		this.poll.remove(sock);
		SocketClose(sock);
		this.buffers = buf;
		this.Neighbours = new_list;
	}

	AddSocket(sockfd, ip, direction) {
		var ctx = new SyncContext(ip, direction, sockfd);
		var key = "" + direction + "/" + ip;
		var old_sock = Neighbours[key];
		if (old_sock)
			RemoveSocket(old_sock);

		Neighbours[key] = sockfd;
		this.poll.add(sockfd);

		buffers["" + sockfd] = ctx;
		return ctx;
	}

	ValidHost(sockfd) {
		var info = SocketInfo(sockfd);
		if (info) {
			var ip = info["address"];
			if (ValidIP(ip)) {
				if (TIN_CLUSTER_LOG)
					this.Log("Connected input node $ip");
				AddSocket(sockfd, ip, 0);
				return true;
			}
		}
		return false;
	}

	Dispatch(sock) {
		var res = SocketRead(sock, var buffer, READ_BUFFER);
		if (res > 0) {
			var ctx = buffers["" + sock];
			if (!ctx) {
				RemoveSocket(sock);
				return;
			}
			var buf = ctx.buffer + buffer;
			while (buf) {
				var to_read = FromSize(buf, var bytes);
				if (to_read <= MAX_OP_SIZE) {
					if ((to_read > 0) && (to_read <= length buf - bytes)) {
						var buf2 = SubStr(buf, bytes, to_read);
						buf = SubStr(buf, to_read + bytes);
						if (SocketWrite(sock, "\x02OK") == 3) {
							localjournal.Log(buf2, "sync", opid++, 1);
							NotWritten[length NotWritten] = uncompress(buf2);
						}
					} else {
						ctx.buffer = buf;
						break;
					}
					ctx.buffer = buf;
				} else {
					RemoveSocket(sock);
					break;
				}
			}
		} else {
			RemoveSocket(sock);
		}
	}

	WriteSync() {
		var wait_confirmation = NotConfirmed;
		for (var i = 0; i < length NotWritten; i++) {
			var buf = NotWritten[i];
			if (buf) {
				if (Execute(buf))
					wait_confirmation[length wait_confirmation] = buf;
				else
					localjournal.Log(buf, "sync", opid++, 3);
			}
		}
		NotWritten = new [];
		NotConfirmed = wait_confirmation;
		if (!wait_confirmation)
			localjournal.Reset();
	}

	ConfirmMessage(written_buf_hash) {
		var wait_confirmation = new [];
		
		for (var i = 0; i < length NotConfirmed; i++) {
			var buf = NotConfirmed[i];
			if (buf) { 
				if ("QSHA:" + sha1(buf) == written_buf_hash) {
					localjournal.Log(buf, "sync", opid++, 3);
					if (TIN_CLUSTER_LOG)
						this.Log("ACK $written_buf_hash");
				} else
					wait_confirmation[length wait_confirmation] = buf;
			}
		}
		NotConfirmed = wait_confirmation;
		if (!wait_confirmation)
			localjournal.Reset();
	}

	LoadJournal(data) {
		var target = new [];
		if (data) {
			var buffers = new [];
			for (var i = 0; i < length data; i++) {
				var e = data[i];
				if (e) {
					var buf = e["data"];
					var flags = e["flags"];
					if (buf) {
						var key = sha1(buf);
						if (flags == 3)
							buffers[key] = "";
						else
							buffers[key] = buf;
					}
				}
			}
			for (i = 0; i < length buffers; i++) {
				buf = buffers[i];
				if (buf)
					target[length target] = buf;
			}
			if (target)
				echo "${length target} records to sync\n";
		}
		return target;
	}

	PollSockets() {
		var Socket = this.ServerSocket;
		var server_socket = Socket.Socket;
		var outsockets = this.poll.wait(0);
		if (outsockets) {
			for (var i = 0; i < length outsockets; i++) {
				var socket = outsockets[i];
				if (socket == server_socket) {
					try {
						var client = Socket.Accept(true);
						if (ValidHost(client)) {
							SocketSetOption(client, SOL_SOCKET, SO_RCVTIMEO, 10000);
							SocketSetOption(client, SOL_SOCKET, SO_SNDTIMEO, 10000);
						} else
							SocketClose(client);
					} catch (var exc) {
						echo "SyncWorker: $exc\n";
					}
				} else
					Dispatch(socket);
			}
		}
	}

	QuerySyncWorker(n) {
		current_worker = CurrentWorker();
		journal = new Journal("sync");
		journal.AutoClean = false;

		localjournal = new Journal("syncwrite");
		localjournal.AutoClean = false;

		echo "Checking sync journal ...\n";
		NotSynced = LoadJournal(journal.Read(0, true));
		echo "Checking local sync journal ...\n";
		NotWritten = LoadJournal(localjournal.Read(0, true));
		echo "Done\n";

		n = UnBinarizeObject(n);
		QueryWorkers = n[0];
		var port = n[1];
		var interface = n[2];
		
		var Socket = new TCPSocket();
		if (Socket.Listen(port, SYNC_CONNECTIONS_MAX, interface)) {
			throw "Error initializing TinDB sync socket (port ${port} already in use)";
			panicexit(-1);
		}

		opid = RandomInteger(1, 0xFFFF);
		this.poll = new Poll();
		var server_socket = Socket.Socket;
		this.poll.add(server_socket);
		this.ServerSocket = Socket;
		var timeout = 100;
		while (true) {
			var data = null;
			this.PollSockets();
			if ((Worker::PendingAll(data, timeout, WORKER_POLL_TIMEOUT)) && (data)) {
				for (var i = 0; i < length data; i++) {
					try {
						var data_i = data[i];
						var data_container = UnBinarizeObject(data_i);
						if (typeof data_container == "array") {
							// is confirmation
							var confirmation_buffer = data_container[1];
							FromSize(confirmation_buffer, var bytes);
							var obj = UnBinarizeObject(confirmation_buffer, bytes);
							if ((obj) && (classof obj == "R"))
								ConfirmMessage(obj.q);
						} else {
							if ((classof data_container == "Q") && (!data_container.origp)) {
								data_container.origp = NodeName;
								data_i = BinarizeObject(data_container);
							}
							NotifyNeighbours(data_i);
						}
					} catch (var exc) {
						echo "SyncWorker: $exc\n";
					}
				}
			}
			if (time() > RingTimeout)
				LoadRing();
			// try to sync
			Sync();
			WriteSync();
		}
	}
}

class TinDB {
	protected var[] QueryWorkers;
	protected var WorkerIndex = 0;
	protected var Socket;
	protected var[] StorageWorkers;
	protected var[] IDLEWorkers;
	protected var[] AggregatorWorkers;
	protected var SyncWorker;
	protected var JSWorker;

	Rotate(array, right) {
		if ((right <= 0) || (!array) || (length array <= 1))
			return array;

		right = right % length array;
		if (!right)
			return array;

		var[] new_arr;
		for (var i = right; i < length array; i++)
			new_arr[length new_arr] = array[i];
		for (i = 0; i < right; i++)
			new_arr[length new_arr] = array[i];

		return new_arr;
	}

	TinDB(machineid = 1, sync_port = TIN_SYNCPORT, sync_interface = "", workers = 4, storage_workers = 4, idle_workers = 1, aggregator_workers = 2) {
		RandomSeed(unpack("u32", CryptoRandom(4))[0]);
		var[] sworkers;
		var[] qworkers;
		var[] aworkers;
		var[] idleworkers;

		var[] all_storage_workers;
		var context = Worker::SharedContext();
		for (var i = 0; i < storage_workers; i++) {
			StorageWorkers[i] = new Worker("StorageWorker", "journal${Journal::LeadingZero(length all_storage_workers)}", context);
			var w_obj = StorageWorkers[i]._workerobj;
			sworkers[length sworkers] = w_obj;
			all_storage_workers[length all_storage_workers] = w_obj;
			// Sleep(100);
		}
		for (i = 0; i < idle_workers; i++) {
			IDLEWorkers[i] = new Worker("StorageWorker", "journal${Journal::LeadingZero(length all_storage_workers)}", context);
			var idle_obj = IDLEWorkers[i]._workerobj;
			idleworkers[length idleworkers] = idle_obj;
			all_storage_workers[length all_storage_workers] = idle_obj;
			// Sleep(100);
		}
		for (i = 0; i < workers; i++) {
			QueryWorkers[i] = new Worker("QueryWorker", BinarizeObject(["QW${i+1}", Rotate(sworkers, i), Rotate(idleworkers, i)]), context);
			var q_obj = QueryWorkers[i]._workerobj;
			qworkers[length qworkers] = q_obj;
			// Sleep(100);
		}
		for (i = 0; i < aggregator_workers; i++) {
			AggregatorWorkers[i] = new Worker("QueryAggregator", BinarizeObject(["QAW${i+1}", Rotate(qworkers, i)]), context);
			var a_obj = AggregatorWorkers[i]._workerobj;
			aworkers[length aworkers] = a_obj;
			// Sleep(100);
		}

		JSWorker = new Worker("JSQueryWorker",  BinarizeObject(["QJSW1", sworkers, idleworkers]), context);
		// Sleep(100);

		var sync_worker = null;
		if (TIN_CLUSTER) {
			SyncWorker = new Worker("QuerySyncWorker",  BinarizeObject([qworkers, sync_port, sync_interface]), context);
			sync_worker = SyncWorker._workerobj;
		}

		AddWorkerData(JSWorker._workerobj, BinarizeObject([aworkers, qworkers, sync_worker]));

		for (i = 0; i < length QueryWorkers; i++)
			AddWorkerData(QueryWorkers[i]._workerobj, BinarizeObject([Rotate(aworkers, i), qworkers, sync_worker]));

		var storage_bin = BinarizeObject([all_storage_workers, JSWorker._workerobj, sync_worker, machineid]);
		for (i = 0; i < length all_storage_workers; i++)
			AddWorkerData(all_storage_workers[i], storage_bin);
	}

	static Check() {
		return Journal::CheckAll("journal");
	}

	static InitializeDatabases() {
		var AccessDB = new TinBase("sys");
		var t = AccessDB["users"];
		// t.Lock(true);
		if (!t.EnsureIndex("username"))
			t.Store(["username" => "sysdbadministrator", "password" => sha256("sysdbadministrator"), "db" => ""]);
		// t.Unlock(true);
		t.Flush();
		AccessDB.Reset();
	}

	Start(port = TIN_PORT, max_connections = 1024, working_directory = "", interface = "") {
		if (working_directory)
			_chdir(working_directory);
		Socket = new TCPSocket();
		if (Socket.Listen(port, max_connections, interface))
			throw "Error initializing TinDB server (port ${port} already in use)";

		while (true) {
			var sock = Socket.Accept(true);
			if (sock > 0) {
				var info = SocketInfo(sock);
				SocketSetOption(sock, SOL_SOCKET, SO_RCVTIMEO, 10000);
				SocketSetOption(sock, SOL_SOCKET, SO_SNDTIMEO, 10000);
				var addr = info["address"];
				if ((addr) && ((addr == "127.0.0.1") || (addr == "::1"))) {
					QueryWorkers[WorkerIndex++].AddData(BinarizeObject([sock]));
					if (WorkerIndex >= length QueryWorkers)
						WorkerIndex = 0;
				} else {
					SocketClose(sock);
				}
			}
		}
	}
}

class Main {
	RealPath(path) {
		var res = "";
		path = StrReplace(path, "\\", "/");
		var arr = StrSplit(path, "/");
		for (var i = length arr - 1; i >= 0; i--) {
			var e = arr[i];
			if (e == "..") {
				i--;
			} else {
				if (res)
					res = arr[i] + "/" + res;
				else
					res = arr[i];
			}
		}
		if (path[0] == "/")
			res = "/" + res;
		return res;
	}

	IniGetPath(ini_file, category, key, default_value = "") {
		var res = trim(IniGet(ini_file, category, key, default_value));
		if (res[0] == ".")
			return getcwd() + "/" + res;
		return res;
	}

	Main() {
		echo "Initializing Tinbase...\n";
		var ini_file = "/usr/local/etc/concept.ini";
		if (!FileExists(ini_file))
			ini_file="/usr/etc/concept.ini";
		if (!FileExists(ini_file))
			ini_file="/etc/concept.ini";
		if (!FileExists(ini_file))
			ini_file=getcwd()+"/concept.ini";

		var user = IniGet(ini_file, "TinDB", "User", "");
		var machineid = value IniGet(ini_file, "TinDB", "MachineID", "1");
		var password = "";
		if (user) {
			password = IniGet(ini_file, "TinDB", "UserPassword", "");
		} else {
			user = IniGet(ini_file, "Server", "User", "");
			password = IniGet(ini_file, "Server", "UserPassword", "");
		}
		if (user) {
			if (!SetCurrentUser(user, password))
				echo "TinDB WARNING: cannot change user to $user\n";
		}
		var default_db_path = "/var/lib/concept";
		if (ON_MSWINDOWS)
			default_db_path = "../db";
		var root = RealPath(IniGetPath(ini_file, "Paths", "DB", default_db_path));
		_mkdir(root);
		if ((!root) || (_chdir(root))) {
			echo "Error initializing DB engine\nCannot chdir to '$root'\n";
			return -1;
		}
		echo "Performing database check ...\n";
		if (!TinDB::Check()) {
			echo "Database seems to be corrupted.\n";
			return -1;
		}
		echo "Done, database seems to be OK\n";
		TinDB::InitializeDatabases();
		var db = new TinDB(machineid);
		echo "Listening\n";
		db.Start();
	}
}
